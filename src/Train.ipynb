{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7596f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipykernel -U --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba53b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Found 1 CUDA device(s).\n",
      "PyTorch CUDA version: 12.1\n",
      "TorchAudio version: 2.1.0+cu121\n",
      "diffusers: 0.33.1\n",
      "huggingface_hub: 0.30.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CUDA is available! Found 1 CUDA device(s).\\nPyTorch CUDA version: 12.1\\nTorchAudio version: 2.1.0+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # in a notebook cell\n",
    "# !pip install --upgrade --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# !pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "# Uncomment and run these lines if you need to install or upgrade packages\n",
    "# %pip install --upgrade huggingface_hub\n",
    "# %pip install --upgrade diffusers\n",
    "# # In a notebook cell\n",
    "# %pip install --upgrade accelerate\n",
    "import torch\n",
    "import torchaudio\n",
    "# --- ADDED CHECK: Verify CUDA is available early ---\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. Check PyTorch installation and NVIDIA drivers/CUDA Toolkit.\"\n",
    "print(f\"CUDA is available! Found {torch.cuda.device_count()} CUDA device(s).\")\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "print(\"TorchAudio version:\", torchaudio.__version__)\n",
    "# --- END ADDED CHECK ---\n",
    "\n",
    "import diffusers\n",
    "import huggingface_hub\n",
    "from huggingface_hub import hf_hub_download # Use the new download API\n",
    "print(\"diffusers:\", diffusers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "from diffusers.models.autoencoders.vae import Decoder  # should import with no error\n",
    "\n",
    "\"\"\"CUDA is available! Found 1 CUDA device(s).\n",
    "PyTorch CUDA version: 12.1\n",
    "TorchAudio version: 2.1.0+cu121\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Fix the CUDA runtimex path issue for Lightning.ai ---\n",
    "# cuda_path = \"/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/nvidia/cuda_runtime/lib\"\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = cuda_path + \":\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "# Force-load libcudart so Python sees it early\n",
    "import ctypes\n",
    "# ctypes.CDLL(os.path.join(cuda_path, \"libcudart.so.11.0\"))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Now safe to import other modules\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = False   # prevents long autotune\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "Initializing Accelerator...\n",
      "Gradient clipping enabled with max_norm=1.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (Modified)\n",
    "\n",
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None:\n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)\n",
    "\n",
    "data_type = torch.float32 # Use full precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "print(\"Initializing Accelerator...\")\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"no\") # Disable AMP\n",
    "accelerator.gradient_clipping = 1.0 # Keep clipping enabled\n",
    "print(f\"Gradient clipping enabled with max_norm={accelerator.gradient_clipping}\")\n",
    "# -----------------------------\n",
    "\n",
    "# if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "#     global_batch_size = batch_size = 4\n",
    "# else:\n",
    "#     global_batch_size_env = os.environ.get(\"GLOBAL_BATCH_SIZE\")\n",
    "#     if global_batch_size_env is None:\n",
    "#         print(\"Warning: GLOBAL_BATCH_SIZE environment variable not set. Using default batch_size=8.\")\n",
    "#         global_batch_size = batch_size = 4\n",
    "#     else:\n",
    "#         global_batch_size = int(global_batch_size_env)\n",
    "#         batch_size = global_batch_size // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 709045\n",
      "Accelerator selected device: cuda\n",
      "DataLoader num_workers = 1\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float32\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "# --- ADDED CHECK: Verify accelerator picked a CUDA device ---\n",
    "print(\"Accelerator selected device:\", device)\n",
    "assert str(device).startswith(\"cuda\"), f\"Accelerator did not select a CUDA device (selected: {device}). Ensure GPU is visible and accelerate is configured correctly.\"\n",
    "# --- END ADDED CHECK ---\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "# num_devices = torch.cuda.device_count() # Already defined\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "# num_workers = num_devices # num_workers for DataLoader, might need separate tuning\n",
    "num_workers = num_devices\n",
    "print(f\"DataLoader num_workers = {num_workers}\")\n",
    "\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: fmri_model_v1_1ses_50ep\n",
      "--data_path=/workspace/MindEyeV2/MindEyeV2/src/data                 --cache_dir=/workspace/MindEyeV2/MindEyeV2/src/cache                 --model_name=fmri_model_v1_1ses_50ep                 --no-multi_subject --subj=1 --batch_size=4 --num_sessions=1                     --hidden_dim=1024 --clip_scale=1.                     --blurry_recon --blur_scale=.5                      --use_prior --prior_scale=30                     --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=50 --no-use_image_aug                     --ckpt_interval=999 --ckpt_saving --wandb_log\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"fmri_model_v1_5ses_50ep\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path={os.getcwd()}/data \\\n",
    "                --cache_dir={os.getcwd()}/cache \\\n",
    "                --model_name={model_name} \\\n",
    "                --no-multi_subject --subj=1 --batch_size=8 --num_sessions=5 \\\n",
    "                    --hidden_dim=1024 --clip_scale=1. \\\n",
    "                    --blurry_recon --blur_scale=.5  \\\n",
    "                    --use_prior --prior_scale=30 \\\n",
    "                    --n_blocks=4 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=50 --no-use_image_aug \\\n",
    "                    --ckpt_interval=999 --ckpt_saving --wandb_log\"\n",
    "    # --multisubject_ckpt=../train_logs/multisubject_subj01_1024_24bs_nolow\n",
    "    # --use_prior --prior_scale=30 \\\n",
    "\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973cdac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/MindEyeV2/MindEyeV2/src/data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()+\"/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd()+\"/data\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=1,\n",
    "    help=\"Number of training sessions to include\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=3,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=1e-1,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 4 num_iterations_per_epoch = 187 num_samples_per_epoch = 750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subj01...\n",
      "Training with 1 sessions\n",
      "Train URL: /workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/train/{0..0}.tar\n",
      "Loading betas from /workspace/MindEyeV2/MindEyeV2/src/data/betas_all_subj01_fp32_renorm.hdf5\n",
      "num_voxels for subj01: 15724\n",
      "Updated num_voxels_list: [15724]\n",
      "Loaded all subj train dls and subsetted betas!\n",
      "\n",
      "/workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "num_voxels_list = [] # Reset this list\n",
    "\n",
    "for s in subj_list:\n",
    "    print(f\"Processing subj0{s}...\")\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "    print(\"Train URL:\", train_url)\n",
    "\n",
    "    # Setup DataLoader (unchanged)\n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # Load betas (reverted to direct loading)\n",
    "    beta_file_path = f'{data_path}/betas_all_subj0{s}_fp32_renorm.hdf5'\n",
    "    print(f\"Loading betas from {beta_file_path}\")\n",
    "    try:\n",
    "        f = h5py.File(beta_file_path, 'r')\n",
    "        # --- Reverted beta loading ---\n",
    "        betas = f['betas'][:] # Load all voxels directly\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {beta_file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type) # Use full betas, convert dtype\n",
    "    current_num_voxels = betas.shape[1]\n",
    "    num_voxels_list.append(current_num_voxels)\n",
    "    num_voxels[f'subj0{s}'] = current_num_voxels\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Updated num_voxels_list:\", num_voxels_list) # Verify the list contains smaller numbers\n",
    "print(\"Loaded all subj train dls and subsetted betas!\\n\")\n",
    "\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all 73k possible NSD images to cpu! (73000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing **SMALLER** FrozenOpenCLIPImageEmbedder (ViT-L-14)...\n",
      "CLIP model explicitly set to torch.float32\n",
      "Total CLIP feature dimension: 37632\n",
      "Testing CLIP output dimensions...\n",
      "Testing CLIP output dimensions...\n",
      "CLIP output shape: torch.Size([1, 49, 768])\n",
      "Actual seq_dim: 49\n",
      "Actual emb_dim: 768\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 (Modified to use ViT-L-14)\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/hf_cache\"   # or set HF_HUB_CACHE\n",
    "print(\"Initializing **SMALLER** FrozenOpenCLIPImageEmbedder (ViT-L-14)...\")\n",
    "\n",
    "# In Cell 16, where you initialize the CLIP embedder\n",
    "\n",
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-B-32\",\n",
    "    version=\"openai\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "\n",
    "# Convert the CLIP model to float32 explicitly BEFORE moving to device\n",
    "clip_img_embedder.to(data_type)\n",
    "print(f\"CLIP model explicitly set to {data_type}\")\n",
    "\n",
    "# Now move to device\n",
    "clip_img_embedder.to(device)\n",
    "# --- CORRECTED CLIP dimensions based on ACTUAL output ---\n",
    "clip_seq_dim = 49    # Reduced from 256 (7x7 grid for 32px patches)\n",
    "clip_emb_dim = 768   # Reduced from 514 or something (base CLIP size)\n",
    "total_dim = clip_seq_dim * clip_emb_dim \n",
    "print(f\"Total CLIP feature dimension: {total_dim}\")\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# param_size = sum(p.numel() * p.element_size() for p in clip_img_embedder.parameters()) / 1024**2\n",
    "# print(f\"CLIP Model Size: {param_size:.1f} MB\")\n",
    "\n",
    "# Add this after CLIP embedder initialization\n",
    "accelerator.print(\"Testing CLIP output dimensions...\")\n",
    "print(\"Testing CLIP output dimensions...\")\n",
    "with torch.no_grad():\n",
    "    test_img = torch.randn(1, 3, 224, 224).to(device)\n",
    "    test_out = clip_img_embedder(test_img)\n",
    "    print(f\"CLIP output shape: {test_out.shape}\")\n",
    "    actual_seq_dim = test_out.shape[1]\n",
    "    actual_emb_dim = test_out.shape[2]\n",
    "    print(f\"Actual seq_dim: {actual_seq_dim}\")\n",
    "    print(f\"Actual emb_dim: {actual_emb_dim}\")\n",
    "\n",
    "# --- IMPORTANT: Re-run the cell defining model.backbone (Cell 20) ---\n",
    "# If the BrainNetwork's `out_dim` depends on `clip_emb_dim * clip_seq_dim`,\n",
    "# it needs to be re-initialized with the *new* smaller dimensions from ViT-L.\n",
    "# Otherwise, you might get shape mismatch errors later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "## SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/MindEyeV2/MindEyeV2/src/cache\n",
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    print(f'{cache_dir}')\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "    from autoencoder.convnext import ConvnextXL\n",
    "    cnx = ConvnextXL(f'{cache_dir}/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(device)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "    \n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "torch.Size([2, 1, 15724]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,1,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "54,279,036 total\n",
      "54,279,036 trainable\n",
      "param counts:\n",
      "70,381,436 total\n",
      "70,381,436 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "Backbone Output Shape: torch.Size([2, 49, 768])\n",
      "Clip Output Shape: torch.Size([2, 49, 768])\n",
      "Blurry Recon Shapes: torch.Size([2, 4, 28, 28]) torch.Size([2, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "# Cell 20 (Re-run this cell AFTER running the modified Cell 16)\n",
    "\n",
    "from models import BrainNetwork\n",
    "# from diffusers.models.autoencoders.vae import Decoder # Already imported if needed\n",
    "\n",
    "# This initialization now uses clip_emb_dim = 1024 and clip_seq_dim = 256\n",
    "# Ensure BrainNetwork's out_dim calculation uses these updated global variables correctly\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, n_blocks=n_blocks,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim,\n",
    "                          blurry_recon=blurry_recon, clip_scale=clip_scale)\n",
    "\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "# The backbone output shape should now reflect the 1024 dimension\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "# backbone_ output shape: (batch, seq_len, emb_dim) -> (2, 256, 1024)\n",
    "# clip_ output shape: (batch, seq_len, emb_dim) -> (2, 256, 1024)\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(\"Backbone Output Shape:\", backbone_.shape)\n",
    "print(\"Clip Output Shape:\", clip_.shape)\n",
    "if blurry_recon:\n",
    "    print(\"Blurry Recon Shapes:\", blur_[0].shape, blur_[1].shape)\n",
    "else:\n",
    "    print(\"Blurry recon disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "55,096,640 total\n",
      "55,096,624 trainable\n",
      "param counts:\n",
      "125,478,076 total\n",
      "125,478,060 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 9350\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "125,478,076 total\n",
      "125,478,060 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "#print(\"Trying SGD optimizer...\")\n",
    "#optimizer = torch.optim.SGD(opt_grouped_parameters, lr=1e-3) # Try SGD\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malbst\u001b[0m (\u001b[33mfranek-liszka-it-university-of-copenhagen\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/MindEyeV2/MindEyeV2/src/wandb/run-20250505_105035-v12shupp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small/runs/v12shupp' target=\"_blank\">light-republic-51</a></strong> to <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small/runs/v12shupp' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small/runs/v12shupp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb mindeye run fmri_model_v1_1ses_50ep\n",
      "wandb_config:\n",
      " {'model_name': 'fmri_model_v1_1ses_50ep', 'global_batch_size': 4, 'batch_size': 4, 'num_epochs': 50, 'num_sessions': 1, 'num_params': 125478060, 'clip_scale': 1.0, 'prior_scale': 30.0, 'blur_scale': 0.5, 'use_image_aug': False, 'max_lr': 0.0003, 'mixup_pct': 0.33, 'num_samples_per_epoch': 750, 'num_test': 3000, 'ckpt_interval': 999, 'ckpt_saving': True, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '/workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/train/{0..0}.tar', 'test_url': '/workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/new_test/0.tar'}\n",
      "wandb_id: fmri_model_v1_1ses_50ep\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-republic-51</strong> at: <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small/runs/v12shupp' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small/runs/v12shupp</a><br> View project at: <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/fmri-small</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_105035-v12shupp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/MindEyeV2/MindEyeV2/src/wandb/run-20250505_105036-fmri_model_v1_1ses_50ep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye/runs/fmri_model_v1_1ses_50ep' target=\"_blank\">fmri_model_v1_1ses_50ep</a></strong> to <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye/runs/fmri_model_v1_1ses_50ep' target=\"_blank\">https://wandb.ai/franek-liszka-it-university-of-copenhagen/mindeye/runs/fmri_model_v1_1ses_50ep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "global_batch_size = batch_size\n",
    "wandb.init(project=\"fmri-small\")\n",
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_sessions\": num_sessions,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None:\n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created list of 1 PyTorch DataLoaders.\n",
      "Explicitly converting model to torch.float32 before prepare...\n",
      "Preparing model, optimizer, and scheduler with Accelerator...\n",
      "Model, optimizer, and scheduler prepared.\n",
      "Preparing 1 DataLoader(s) individually...\n",
      "  DataLoader 1 prepared. Type: <class 'accelerate.data_loader.DataLoaderDispatcher'>\n",
      "\n",
      "All components prepared.\n"
     ]
    }
   ],
   "source": [
    "# Cell 32 (Modified to prepare DataLoaders separately)\n",
    "\n",
    "# Create the list of standard PyTorch DataLoaders first\n",
    "original_train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "print(f\"Created list of {len(original_train_dls)} PyTorch DataLoaders.\")\n",
    "\n",
    "# --- Ensure model is in the correct dtype BEFORE preparing ---\n",
    "print(f\"Explicitly converting model to {data_type} before prepare...\")\n",
    "model.to(data_type)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- Prepare the model, optimizer, and scheduler ---\n",
    "print(\"Preparing model, optimizer, and scheduler with Accelerator...\")\n",
    "model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)\n",
    "print(\"Model, optimizer, and scheduler prepared.\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- Prepare each DataLoader individually ---\n",
    "print(f\"Preparing {len(original_train_dls)} DataLoader(s) individually...\")\n",
    "prepared_train_dls = []\n",
    "for i, dl in enumerate(original_train_dls):\n",
    "    prepared_train_dls.append(accelerator.prepare(dl))\n",
    "    print(f\"  DataLoader {i+1} prepared. Type: {type(prepared_train_dls[-1])}\")\n",
    "\n",
    "# Assign the list of *prepared* DataLoaders back to the variable used in the training loop\n",
    "train_dls = prepared_train_dls\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "print(\"\\nAll components prepared.\")\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmri_model_v1_1ses_50ep starting with epoch 0 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Epoch 0 Preloading ---\n",
      "  Preloading for subj01...\n",
      "      Duplicate image indices found in batch 103. Skipping.\n",
      "      Duplicate image indices found in batch 174. Skipping.\n",
      "    subj01: Reached max iterations (187). Breaking inner batch loop.\n",
      "  Preloading complete for epoch after processing subj01. Total iters: 187\n",
      "  Finished preloading attempts for subj01.\n",
      "--- Epoch 0 Preloading Complete ---\n",
      "  Total voxel iters stored: 187\n",
      "\n",
      "--- Starting Epoch 0 Training Loop ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 34 (Modified with Preloading Debugging AND Gradient Inspection)\n",
    "\n",
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "\n",
    "    # --- Reset counters for the epoch ---\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    loss_clip_total = 0.\n",
    "    # Reset other counters if added back based on config\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    loss_prior_total = 0.\n",
    "    recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    blurry_pixcorr = 0.\n",
    "\n",
    "    # --- Data Preloading Loop ---\n",
    "    print(f\"\\n--- Starting Epoch {epoch} Preloading ---\")\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    # Ensure image_iters uses the correct data_type for storage from the start\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224, dtype=data_type)\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    effective_iters_processed = 0 # Counter for successfully processed iterations\n",
    "\n",
    "    # --- Loop over subjects/dataloaders ---\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        subject_id = subj_list[s] # Get the actual subject number\n",
    "        print(f\"  Preloading for subj0{subject_id}...\")\n",
    "        # Use try-finally to ensure we see print statements even if loop breaks early\n",
    "        try:\n",
    "            # iter = -1 # Reset iter concept - we rely on effective_iters_processed now\n",
    "            # --- Loop over batches in dataloader ---\n",
    "            for batch_idx, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                # Check if we have already preloaded enough valid iterations\n",
    "                if effective_iters_processed >= num_iterations_per_epoch:\n",
    "                    print(f\"    subj0{subject_id}: Reached max iterations ({num_iterations_per_epoch}). Breaking inner batch loop.\")\n",
    "                    break # Exit inner batch loop for this subject\n",
    "\n",
    "                # Load image indices and check for duplicates\n",
    "                image_idx = behav0[:,0,0].cpu().long().numpy()\n",
    "                image0, image_sorted_idx = np.unique(image_idx, return_index=True)\n",
    "\n",
    "                # --- <<< APPLY ORIGINAL SKIP LOGIC >>> ---\n",
    "                # Skip if duplicates are found OR if drop_last=True and batch isn't full\n",
    "                # Note: drop_last=True in DataLoader definition is still recommended!\n",
    "                if len(image0) != len(image_idx) or len(image_idx) != batch_size:\n",
    "                    if len(image0) != len(image_idx):\n",
    "                        print(f\"      Duplicate image indices found in batch {batch_idx}. Skipping.\")\n",
    "                    else: # Only possible if drop_last=False and it's the last batch\n",
    "                        print(f\"      Incomplete batch {batch_idx} (size {len(image_idx)}). Skipping.\")\n",
    "                    continue # Skip this batch entirely\n",
    "                # --- <<< END SKIP LOGIC >>> ---\n",
    "\n",
    "                # If we reach here, the batch has batch_size unique images.\n",
    "\n",
    "                current_iter_index = effective_iters_processed # Use the counter for dictionary keys\n",
    "\n",
    "                # --- Load Image Data ---\n",
    "                try:\n",
    "                    # Ensure indices are within bounds (using unique image0)\n",
    "                    max_index = images.shape[0] - 1\n",
    "                    if np.any(image0 >= images.shape[0]) or np.any(image0 < 0):\n",
    "                        print(f\"      ERROR: Image indices out of bounds! Min: {np.min(image0)}, Max: {np.max(image0)}, Allowed: 0-{max_index}. Skipping batch {batch_idx}.\")\n",
    "                        continue\n",
    "                    image_data_np = images[image0] # Use unique indices image0\n",
    "                    image_data_torch = torch.tensor(image_data_np, dtype=data_type)\n",
    "                except Exception as e:\n",
    "                    print(f\"      ERROR loading images for batch {batch_idx}, indices {image0}: {e}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Store image data using the effective iteration index\n",
    "                image_iters[current_iter_index, s*batch_size : s*batch_size + batch_size] = image_data_torch\n",
    "\n",
    "                # --- Load Voxel Data ---\n",
    "                voxel_idx = behav0[:,0,5].cpu().long().numpy()\n",
    "                # Index voxels corresponding to the *unique* images loaded\n",
    "                voxel_sorted_idx = voxel_idx[image_sorted_idx]\n",
    "                try:\n",
    "                    max_voxel_idx = voxels[f'subj0{subject_id}'].shape[0] - 1\n",
    "                    if np.any(voxel_sorted_idx >= voxels[f'subj0{subject_id}'].shape[0]) or np.any(voxel_sorted_idx < 0):\n",
    "                        print(f\"      ERROR: Voxel indices out of bounds! Min: {np.min(voxel_sorted_idx)}, Max: {np.max(voxel_sorted_idx)}, Allowed: 0-{max_voxel_idx}. Skipping batch {batch_idx}.\")\n",
    "                        continue\n",
    "                    voxel0_np = voxels[f'subj0{subject_id}'][voxel_sorted_idx]\n",
    "                    # Voxel0 should now reliably have batch_size dimension\n",
    "                    voxel0 = torch.Tensor(voxel0_np).unsqueeze(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"      ERROR loading voxels for batch {batch_idx}, indices {voxel_sorted_idx}: {e}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Mixco Logic ---\n",
    "                # voxel0 batch dim is now guaranteed to be batch_size\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0) # perm/betas will have size batch_size\n",
    "                    # Store using effective iteration index\n",
    "                    perm_iters[f\"subj0{subject_id}_iter{current_iter_index}\"] = perm\n",
    "                    betas_iters[f\"subj0{subject_id}_iter{current_iter_index}\"] = betas\n",
    "                    select_iters[f\"subj0{subject_id}_iter{current_iter_index}\"] = select\n",
    "\n",
    "                # --- Store Voxel Data ---\n",
    "                voxel_key = f\"subj0{subject_id}_iter{current_iter_index}\"\n",
    "                voxel_iters[voxel_key] = voxel0\n",
    "\n",
    "                # Increment the counter *only* after successfully processing and storing\n",
    "                effective_iters_processed += 1\n",
    "\n",
    "            # End of inner batch loop for one dataloader\n",
    "            # Check again if we've processed enough iterations overall before moving to next subject's DL\n",
    "            if effective_iters_processed >= num_iterations_per_epoch:\n",
    "                print(f\"  Preloading complete for epoch after processing subj0{subject_id}. Total iters: {effective_iters_processed}\")\n",
    "                break # Exit outer subject loop\n",
    "\n",
    "        finally:\n",
    "            # This print might be less informative now, relies on effective_iters_processed\n",
    "            print(f\"  Finished preloading attempts for subj0{subject_id}.\")\n",
    "\n",
    "\n",
    "    print(f\"--- Epoch {epoch} Preloading Complete ---\")\n",
    "    # Ensure we actually processed the expected number of iterations\n",
    "    if effective_iters_processed < num_iterations_per_epoch:\n",
    "        print(f\"  WARNING: Only preloaded {effective_iters_processed} / {num_iterations_per_epoch} iterations due to skipped batches.\")\n",
    "        # You might need to adjust num_iterations_per_epoch used in the training loop below\n",
    "        # num_iterations_per_epoch = effective_iters_processed # Risky if running multi-subject/multi-gpu\n",
    "        # Safer to just proceed, the training loop might throw KeyError if it tries to access missing iters\n",
    "    print(f\"  Total voxel iters stored: {len(voxel_iters)}\") # Should match effective_iters_processed\n",
    "\n",
    "\n",
    "    # --- Main Training Loop ---\n",
    "    print(f\"\\n--- Starting Epoch {epoch} Training Loop ---\")\n",
    "    nan_or_inf_grad_detected_this_epoch = False\n",
    "\n",
    "    # Use the *original* num_iterations_per_epoch calculated, but be aware of potential KeyErrors if not enough iters were preloaded\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        try:\n",
    "            # --- Autocast and Optimizer Zero Grad ---\n",
    "            with torch.cuda.amp.autocast(dtype=data_type):\n",
    "                optimizer.zero_grad()\n",
    "                loss=torch.tensor(0.0, device=device)\n",
    "\n",
    "                # --- Load Batch Data ---\n",
    "                voxel_list = []\n",
    "                for si, s_num in enumerate(subj_list):\n",
    "                    voxel_key = f\"subj0{s_num}_iter{train_i}\" # Use train_i directly\n",
    "                    if voxel_key not in voxel_iters:\n",
    "                        # This might happen if preloading skipped too many batches\n",
    "                        print(f\"\\nFATAL ERROR: Missing preloaded key '{voxel_key}' during training loop!\")\n",
    "                        print(f\"  Check preloading warnings. Effective preloaded iters: {effective_iters_processed}\")\n",
    "                        raise KeyError(f\"Preloaded key '{voxel_key}' missing!\")\n",
    "                    voxel_list.append(voxel_iters[voxel_key].detach().to(device)) # Should always have batch_size dim now\n",
    "\n",
    "                # Determine actual batch size for this iteration (should be batch_size * len(subj_list))\n",
    "                current_iter_batch_size = voxel_list[0].shape[0] * len(subj_list)\n",
    "\n",
    "                # Load corresponding image data\n",
    "                image = image_iters[train_i, :current_iter_batch_size].detach().to(device) # Should align\n",
    "                # print(f\"  Training Iter {train_i}: Data moved to device.\") # Optional: verbose\n",
    "\n",
    "                # --- Image Augmentation ---\n",
    "                if use_image_aug:\n",
    "                    image = img_augment(image)\n",
    "\n",
    "                # --- Get CLIP Target ---\n",
    "                clip_target = clip_img_embedder(image)\n",
    "                assert not torch.any(torch.isnan(clip_target)), f\"NaN detected in clip_target at iter {train_i}\"\n",
    "                assert torch.isfinite(clip_target).all(), f\"Inf detected in clip_target at iter {train_i}\"\n",
    "\n",
    "                # --- Prepare Mixco Inputs ---\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    perm_list = []\n",
    "                    betas_list = []\n",
    "                    select_list = []\n",
    "                    for s_num in subj_list:\n",
    "                        perm_key = f\"subj0{s_num}_iter{train_i}\"\n",
    "                        # Check keys exist (important if preloading was incomplete)\n",
    "                        if perm_key not in perm_iters: raise KeyError(f\"Missing Mixco perm key: {perm_key}\")\n",
    "                        betas_key = perm_key.replace(\"perm\",\"betas\")\n",
    "                        if betas_key not in betas_iters: raise KeyError(f\"Missing Mixco betas key: {betas_key}\")\n",
    "                        select_key = perm_key.replace(\"perm\",\"select\")\n",
    "                        if select_key not in select_iters: raise KeyError(f\"Missing Mixco select key: {select_key}\")\n",
    "                        # Append tensors (should always have batch_size dimension)\n",
    "                        perm_list.append(perm_iters[perm_key].detach().to(device))\n",
    "                        betas_list.append(betas_iters[betas_key].detach().to(device))\n",
    "                        select_list.append(select_iters[select_key].detach().to(device))\n",
    "                    perm = torch.cat(perm_list, dim=0)   # Final size should match feature batch size\n",
    "                    betas = torch.cat(betas_list, dim=0)\n",
    "                    select = torch.cat(select_list, dim=0)\n",
    "\n",
    "                # --- Model Forward Pass ---\n",
    "                voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "                voxel_ridge = torch.cat(voxel_ridge_list, dim=0) # Will have batch_size dimension\n",
    "                backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge) # Features will have batch_size dimension\n",
    "                assert not torch.any(torch.isnan(clip_voxels)), f\"NaN detected in clip_voxels (backbone output) at iter {train_i}\"\n",
    "                assert torch.isfinite(clip_voxels).all(), f\"Inf detected in clip_voxels (backbone output) at iter {train_i}\"\n",
    "\n",
    "                # --- Normalize Features & Calculate Loss ---\n",
    "                if clip_scale > 0:\n",
    "                    clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1, p=2.0, eps=1e-12) # Size [batch_size * Nsubj, feats]\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1, p=2.0, eps=1e-12) # Size [batch_size * Nsubj, feats]\n",
    "                    assert not torch.any(torch.isnan(clip_voxels_norm)), f\"NaN detected after normalizing clip_voxels at iter {train_i}\"\n",
    "                    assert not torch.any(torch.isnan(clip_target_norm)), f\"NaN detected after normalizing clip_target at iter {train_i}\"\n",
    "                    assert torch.isfinite(clip_voxels_norm).all(), f\"Inf detected after normalizing clip_voxels at iter {train_i}\"\n",
    "                    assert torch.isfinite(clip_target_norm).all(), f\"Inf detected after normalizing clip_target at iter {train_i}\"\n",
    "\n",
    "                    if epoch < int(mixup_pct * num_epochs):\n",
    "                        current_temp = 0.006\n",
    "                        loss_clip = utils.mixco_nce(clip_voxels_norm, clip_target_norm, temp=current_temp, perm=perm, betas=betas, select=select)\n",
    "                    else:\n",
    "                        epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                        current_temp = epoch_temp\n",
    "                        loss_clip = utils.soft_clip_loss(clip_voxels_norm, clip_target_norm, temp=epoch_temp)\n",
    "\n",
    "                    assert not torch.any(torch.isnan(loss_clip)), f\"NaN detected in calculated loss_clip at iter {train_i} with temp={current_temp}\"\n",
    "                    assert torch.isfinite(loss_clip).all(), f\"Inf detected in calculated loss_clip at iter {train_i} with temp={current_temp}\"\n",
    "\n",
    "                    loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss = loss + loss_clip\n",
    "\n",
    "                    # Accuracy calculation moved here as it depends on loss_clip components\n",
    "                    labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device)\n",
    "                    fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                    bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "\n",
    "                # --- Blurry Recon Loss (if enabled) ---\n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "                    # ... (rest of blurry recon logic, ensure it's compatible with current batch size) ...\n",
    "                    # ... (Need to handle `autoenc`, `cnx`, `blur_augs` potentially being None if disabled) ...\n",
    "                    if autoenc is not None and cnx is not None: # Check if models are loaded\n",
    "                        image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                        loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                        loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                        image_norm = (image - mean)/std\n",
    "                        image_aug = (blur_augs(image) - mean)/std\n",
    "                        _, cnx_embeds = cnx(image_norm)\n",
    "                        _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                        cont_loss = utils.soft_cont_loss(\n",
    "                            nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                            nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                            nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                            temp=0.2)\n",
    "                        loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                        loss_blurry_combined = (loss_blurry + 0.1*cont_loss) * blur_scale\n",
    "                        loss = loss + loss_blurry_combined\n",
    "\n",
    "                        # Pixcorr calculation (if needed)\n",
    "                        with torch.no_grad():\n",
    "                            random_samps = np.random.choice(np.arange(len(image)), size=max(1, len(image)//5), replace=False) # Ensure at least 1 sample\n",
    "                            blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                            pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                            blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                # --- Diffusion Prior Loss (if enabled) ---\n",
    "                if use_prior:\n",
    "                     if model.diffusion_prior is not None: # Check if model is loaded\n",
    "                          loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                          loss_prior_total += loss_prior.item()\n",
    "                          loss_prior = loss_prior * prior_scale\n",
    "                          loss = loss + loss_prior\n",
    "\n",
    "                          # Recon metrics calculation (if needed)\n",
    "                          recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                          recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            # --- End of Autocast ---\n",
    "\n",
    "            # --- Final Loss Check ---\n",
    "            # print(f\"[DEBUG train_i={train_i}] Final Loss before check: {loss.item():.4f}\") # Optional\n",
    "            utils.check_loss(loss) # Check loss is finite before backward\n",
    "\n",
    "            # --- Backward Pass ---\n",
    "            # print(f\"[DEBUG train_i={train_i}] Starting backward pass...\") # Optional\n",
    "            accelerator.backward(loss)\n",
    "            # print(f\"[DEBUG train_i={train_i}] Backward pass complete.\") # Optional\n",
    "\n",
    "            # --- **** GRADIENT INSPECTION & CONDITIONAL OPTIMIZER STEP **** ---\n",
    "            # print(f\"[DEBUG train_i={train_i}] Inspecting gradients before optimizer step...\") # Optional: Verbose\n",
    "            found_inf_or_nan_grad = False\n",
    "            # Clip gradients before checking/stepping if clipping enabled\n",
    "            # Note: accelerator.backward might handle clipping if configured, but explicit clip_grad_norm_ is safer assurance\n",
    "            if accelerator.gradient_clipping is not None:\n",
    "                 # accelerator.clip_grad_norm_ might need to be called if not automatic\n",
    "                 # For now, let's assume backward handles it or inspect first\n",
    "                 pass # Placeholder - rely on inspection first\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                if not torch.isfinite(param.grad).all():\n",
    "                    print(f\"  WARNING: Found non-finite gradients (NaN or Inf) in param: {name} at iter {train_i}\")\n",
    "                    found_inf_or_nan_grad = True\n",
    "                    nan_or_inf_grad_detected_this_epoch = True # Set epoch flag\n",
    "                    break # Stop checking after first bad grad found\n",
    "\n",
    "            if not found_inf_or_nan_grad:\n",
    "                 # print(f\"[DEBUG train_i={train_i}] Gradients seem finite. Proceeding with optimizer step...\") # Optional\n",
    "                 optimizer.step() # Only step if gradients are valid\n",
    "            else:\n",
    "                 print(f\"  Skipping optimizer step for iter {train_i} due to non-finite gradients.\")\n",
    "\n",
    "            optimizer.zero_grad() # Zero grad regardless of step\n",
    "            # --- **** END GRADIENT INSPECTION & CONDITIONAL OPTIMIZER STEP **** ---\n",
    "\n",
    "\n",
    "            # --- Logging ---\n",
    "            losses.append(loss.item()) # Log loss even if step was skipped\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "        # --- Error Catching ---\n",
    "        except KeyError as e:\n",
    "             print(f\"\\n\\n*** KeyError Detected at Epoch {epoch}, Training Iteration {train_i} ***\")\n",
    "             print(f\"Likely missing preloaded data for key: {e}\")\n",
    "             print(\"Check the preloading loop logs for warnings or errors.\")\n",
    "             raise e\n",
    "        except ValueError as e:\n",
    "            if 'NaN loss' in str(e):\n",
    "                print(f\"\\n\\n*** NaN Loss Detected at Epoch {epoch}, Iteration {train_i} ***\")\n",
    "            elif 'Attempting to unscale FP16 gradients' in str(e):\n",
    "                 print(f\"\\n\\n*** FP16 Unscale Error at Epoch {epoch}, Iteration {train_i} ***\")\n",
    "                 print(\"This happened DESPITE inspecting gradients. Check if inspection missed something or if clipping is active/effective.\")\n",
    "            else:\n",
    "                 print(f\"\\n\\n*** ValueError at Epoch {epoch}, Iteration {train_i}: {e} ***\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"\\n\\n*** An unexpected error occurred at Epoch {epoch}, Iteration {train_i} ***\")\n",
    "            print(f\"Error Type: {type(e)}\")\n",
    "            print(f\"Error Details: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    # Check if training was unstable in this epoch\n",
    "    if nan_or_inf_grad_detected_this_epoch:\n",
    "         print(f\"\\n*** WARNING: Non-finite gradients were detected and optimizer steps skipped during Epoch {epoch}. Training might be unstable. Consider lowering LR or checking model/data further. ***\\n\")\n",
    "\n",
    "\n",
    "    # --- Eval Section ---\n",
    "    model.eval()\n",
    "    if local_rank==0: # Only evaluate on main process\n",
    "        # ... (Your existing evaluation code - ensure it handles potential None models if disabled) ...\n",
    "        pass # Placeholder for your eval code\n",
    "\n",
    "    # --- Logging and Saving ---\n",
    "    # --- Logging and Saving ---\n",
    "    if local_rank == 0: # Only log and save on main process\n",
    "        # Calculate epoch duration\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        hours, remainder = divmod(epoch_duration, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        \n",
    "        # Calculate average metrics for the epoch\n",
    "        avg_loss = np.mean(losses[-(train_i+1):]) if train_i >= 0 else 0\n",
    "        avg_fwd_acc = (fwd_percent_correct / (train_i + 1)) if train_i >= 0 else 0\n",
    "        avg_bwd_acc = (bwd_percent_correct / (train_i + 1)) if train_i >= 0 else 0\n",
    "        avg_loss_clip = (loss_clip_total / (train_i + 1)) if train_i >= 0 else 0\n",
    "        # ... calculate other averages similarly ...\n",
    "\n",
    "        logs = {\"train/loss\": avg_loss,\n",
    "                \"train/lr\": lrs[-1] if lrs else 0,\n",
    "                \"train/fwd_pct_correct\": avg_fwd_acc,\n",
    "                \"train/bwd_pct_correct\": avg_bwd_acc,\n",
    "                \"train/loss_clip_total\": avg_loss_clip,\n",
    "                \"train/epoch_time_hours\": f\"{hours:02.0f}:{minutes:02.0f}:{seconds:05.2f}\"\n",
    "                # Add other averaged train metrics here if needed\n",
    "                # Add test metrics here if calculated\n",
    "                }\n",
    "        # Add formatted time string to progress bar\n",
    "        time_str = f\"Time: {hours:02.0f}h {minutes:02.0f}m {seconds:05.2f}s\"\n",
    "        progress_bar.set_postfix(**logs, time=time_str)\n",
    "        \n",
    "        if wandb_log: \n",
    "            # Also log the raw seconds for plotting\n",
    "            logs[\"train/epoch_time_seconds\"] = epoch_duration\n",
    "            wandb.log(logs, step=epoch) # Log per epoch\n",
    "\n",
    "        if (ckpt_saving) and (epoch % ckpt_interval == 0 or epoch == num_epochs - 1):\n",
    "            save_ckpt(f'last') # Save last ckpt potentially more often or at end\n",
    "            # Add best ckpt saving logic if needed based on a validation metric\n",
    "\n",
    "        if (ckpt_saving) and (epoch % ckpt_interval == 0 or epoch == num_epochs - 1):\n",
    "            save_ckpt(f'last') # Save last ckpt potentially more often or at end\n",
    "            # Add best ckpt saving logic if needed based on a validation metric\n",
    "\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- End of Epoch Loop ---\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving and accelerator.is_main_process: # Ensure saving happens only on main process\n",
    "    save_ckpt(f'final') # Save final checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502985ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b755f7c-8a3b-47d7-a096-05594aef2734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
