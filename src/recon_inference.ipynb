{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import torch.nn.functional as F   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "accelerator = Accelerator(split_batches=True, mixed_precision='fp16')\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "tag='last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: fmri_model_v1_1ses_50ep\n",
      "--data_path=/workspace/MindEyeV2/MindEyeV2/src/data                     --cache_dir=/workspace/MindEyeV2/MindEyeV2/src/cache                     --model_name=fmri_model_v1_1ses_50ep --subj=1                     --hidden_dim=1024 --n_blocks=4 --new_test\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"fmri_model_v1_1ses_50ep\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path={os.getcwd()}/data \\\n",
    "                    --cache_dir={os.getcwd()}/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --new_test\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"fmri_model_v1_1ses_50ep\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voxels for subj01: 15724\n",
      "/workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voxels = {}\n",
    "# Load hdf5 data for betas\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32_renorm.hdf5', 'r')\n",
    "betas = f['betas'][:]\n",
    "betas = torch.Tensor(betas).to(\"cpu\")\n",
    "num_voxels = betas[0].shape[-1]\n",
    "voxels[f'subj0{subj}'] = betas\n",
    "print(f\"num_voxels for subj0{subj}: {num_voxels}\")\n",
    "\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "else: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    \n",
    "print(test_url)\n",
    "def my_split_by_node(urls): return urls\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000 3000 1000\n"
     ]
    }
   ],
   "source": [
    "# Prep images but don't load them all to memory\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "\n",
    "# Prep test voxels and indices of test images\n",
    "test_images_idx = []\n",
    "test_voxels_idx = []\n",
    "for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "    test_voxels = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "    test_voxels_idx = np.append(test_images_idx, behav[:,0,5].cpu().numpy())\n",
    "    test_images_idx = np.append(test_images_idx, behav[:,0,0].cpu().numpy())\n",
    "test_images_idx = test_images_idx.astype(int)\n",
    "test_voxels_idx = test_voxels_idx.astype(int)\n",
    "\n",
    "assert (test_i+1) * num_test == len(test_voxels) == len(test_images_idx)\n",
    "print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "Loading blurry recon model\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "50,076,528 total\n",
      "50,076,528 trainable\n",
      "param counts:\n",
      "66,178,928 total\n",
      "66,178,928 trainable\n",
      "param counts:\n",
      "55,096,640 total\n",
      "55,096,624 trainable\n",
      "param counts:\n",
      "121,275,568 total\n",
      "121,275,552 trainable\n",
      "\n",
      "---loading /workspace/MindEyeV2/MindEyeV2/train_logs/fmri_model_v1_1ses_50ep/last.pth ckpt---\n",
      "\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    # arch=\"ViT-L-14\",\n",
    "    arch=\"ViT-B-32\",\n",
    "    version='openai',\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "# clip_img_embedder.to(\"cpu\")\n",
    "clip_img_embedder.to(\"cpu\")\n",
    "clip_seq_dim = 49\n",
    "clip_emb_dim = 768\n",
    "\n",
    "if blurry_recon:\n",
    "    \n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=224,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(\"cpu\")\n",
    "    utils.count_params(autoenc)\n",
    "    print(\"Loading blurry recon model\")\n",
    "    \n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim)\n",
    "\n",
    "from diffusers.models.autoencoders.vae import Decoder\n",
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)\n",
    "# Load pretrained model ckpt\n",
    "outdir = f\"/teamspace/studios/this_studio/MindEyeV2/train_logs/{model_name}\"\n",
    "\n",
    "# With this\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')  # Go up one directory to find train_logs\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---attempting to load /workspace/MindEyeV2/MindEyeV2/train_logs/fmri_model_v1_1ses_50ep/last.pth checkpoint---\n",
      "\n",
      "Checkpoint loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 49\n",
    "clip_text_emb_dim = 768\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "\n",
    "clip_convert = CLIPConverter()\n",
    "\n",
    "# --- FIX: Change the filename here ---\n",
    "# Make sure the file 'epoch8.pth' actually exists in your cache_dir\n",
    "expected_ckpt_path = os.path.join(cache_dir, \"epoch10.pth\")\n",
    "print(f\"\\n---attempting to load {outdir}/{tag}.pth checkpoint---\\n\")\n",
    "checkpoint_path = os.path.join(outdir, f\"{tag}.pth\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    print(f\"WARNING: Directory {outdir} doesn't exist! Creating it...\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"WARNING: Checkpoint file {checkpoint_path} not found!\")\n",
    "    print(\"Continuing without loading a model checkpoint. Results may not be meaningful.\")\n",
    "else:\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        del checkpoint\n",
    "        print(\"Checkpoint loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        try:\n",
    "            import deepspeed\n",
    "            if os.path.exists(os.path.join(outdir, tag)):\n",
    "                state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(\n",
    "                    checkpoint_dir=outdir, tag=tag)\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "                del state_dict\n",
    "                print(\"DeepSpeed checkpoint loaded successfully!\")\n",
    "            else:\n",
    "                print(f\"DeepSpeed checkpoint directory {os.path.join(outdir, tag)} not found!\")\n",
    "        except Exception as deep_e:\n",
    "            print(f\"DeepSpeed loading also failed: {deep_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Looking for unCLIP checkpoint at: /workspace/MindEyeV2/MindEyeV2/src/cache/unclip6_epoch0_step110000.ckpt\n",
      "Found unCLIP checkpoint at: /workspace/MindEyeV2/MindEyeV2/src/cache/unclip6_epoch0_step110000.ckpt\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "# ───── create the engine exactly as before ─────────────────────────\n",
    "diffusion_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config,\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast,\n",
    ")\n",
    "\n",
    "# NEW ↓ — cast weights to fp16 to cut memory in half\n",
    "# diffusion_engine.half()\n",
    "\n",
    "# set to inference and put on GPU\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(\"cpu\")\n",
    "\n",
    "\n",
    "# With these lines\n",
    "ckpt_path = os.path.join(cache_dir, \"unclip6_epoch0_step110000.ckpt\")\n",
    "print(f\"Looking for unCLIP checkpoint at: {ckpt_path}\")\n",
    "\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print(f\"ERROR: unCLIP checkpoint not found at {ckpt_path}\")\n",
    "    print(\"You need to download the unCLIP model checkpoint first.\")\n",
    "    print(\"This is typically available from Hugging Face or the model creator's repository.\")\n",
    "    print(\"After downloading, place it in your cache directory, which is set to:\")\n",
    "    print(f\"  {cache_dir}\")\n",
    "    \n",
    "    # Create required directories\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    \n",
    "    # Optionally raise an exception to stop execution or provide continuation options\n",
    "    raise FileNotFoundError(f\"Missing required checkpoint: {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"Found unCLIP checkpoint at: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    diffusion_engine.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# # recon_inference.ipynb - Main processing cell (REVISED AGAIN)\n",
    "\n",
    "# import inspect\n",
    "# import os # Make sure os is imported if not already\n",
    "# import torch # Make sure torch is imported\n",
    "# import numpy as np # Make sure numpy is imported if not already\n",
    "# import torch.nn.functional as F # Import functional if needed by utils.unclip_recon\n",
    "# from tqdm import tqdm # Ensure tqdm is imported\n",
    "# from torchvision import transforms # Ensure transforms are imported\n",
    "\n",
    "# # --- Ensure blurry_recon argument is used ---\n",
    "# print(f\"Blurry Reconstruction Flag (from args): {blurry_recon}\") # <<< THIS LINE\n",
    "\n",
    "# # (vector_suffix inference logic - keep the corrected version)\n",
    "# try:\n",
    "#     embed_dim = None; embedder_info = []\n",
    "#     if hasattr(diffusion_engine, 'conditioner') and hasattr(diffusion_engine.conditioner, 'embedders'):\n",
    "#         print(\"Available conditioner embedders:\")\n",
    "#         # Correctly iterate over ModuleList\n",
    "#         for i, emb in enumerate(diffusion_engine.conditioner.embedders): # Use enumerate for ModuleList\n",
    "#             emb_type = type(emb); embedder_info.append(f\"  Index: {i}, Type: {emb_type}\")\n",
    "#             # Simplified checks for common types\n",
    "#             if isinstance(emb, FrozenOpenCLIPEmbedder2):\n",
    "#                  # Safely access attributes\n",
    "#                  proj_layer = getattr(getattr(emb, 'model', None), 'text_projection', None)\n",
    "#                  if proj_layer is not None and hasattr(proj_layer, 'shape'):\n",
    "#                       embed_dim = proj_layer.shape[-1]\n",
    "#                       embedder_info[-1] += f\" -> Found embed_dim: {embed_dim} (Preferred)\"\n",
    "#                       break # Found preferred, stop checking\n",
    "#                  else:\n",
    "#                       embedder_info[-1] += \" -> Warning: Could not get text_projection shape\"\n",
    "#             elif isinstance(emb, FrozenOpenCLIPImageEmbedder):\n",
    "#                  found_dim = getattr(emb, 'output_dim', getattr(getattr(emb, 'model', None), 'output_dim', None))\n",
    "#                  if found_dim and embed_dim is None: # Use if no preferred found yet\n",
    "#                      embed_dim = found_dim\n",
    "#                      embedder_info[-1] += f\" -> Found embed_dim: {found_dim}\"\n",
    "#     for info in embedder_info: print(info)\n",
    "#     # Ensure embed_dim is an integer before using it for shape\n",
    "#     vector_suffix_shape = (1, embed_dim) if isinstance(embed_dim, int) else (1, 1024)\n",
    "#     print(f\"Using vector_suffix_shape: {vector_suffix_shape}\")\n",
    "#     vector_suffix = torch.zeros(vector_suffix_shape, device='cpu', dtype=torch.float32)\n",
    "#     print(f\"Created placeholder vector_suffix with shape: {vector_suffix.shape}. Needs verification!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error inferring vector_suffix shape: {e}. Using default (1, 1024).\")\n",
    "#     vector_suffix = torch.zeros(1, 1024, device='cpu', dtype=torch.float32)\n",
    "\n",
    "# # %%\n",
    "# # get all reconstructions\n",
    "# model.to(\"cpu\")\n",
    "# model.eval().requires_grad_(False)\n",
    "\n",
    "# # --- Load Autoencoder Conditionally ---\n",
    "# autoenc = None # Ensure initialized to None\n",
    "# if blurry_recon:\n",
    "#     print(\"Attempting to load Autoencoder for Blurry Recon...\")\n",
    "#     try:\n",
    "#         from diffusers import AutoencoderKL\n",
    "#         autoenc = AutoencoderKL(\n",
    "#              down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "#              up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "#              block_out_channels=[128, 256, 512, 512], layers_per_block=2, sample_size=256,\n",
    "#          )\n",
    "#         ckpt_path = f'{cache_dir}/sd_image_var_autoenc.pth'\n",
    "#         if not os.path.exists(ckpt_path):\n",
    "#              print(f\"ERROR: Autoencoder checkpoint file not found at {ckpt_path}. Disabling blurry recon.\")\n",
    "#              blurry_recon = False # Turn off flag if file missing\n",
    "#              autoenc = None\n",
    "#         else:\n",
    "#              ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "#              autoenc.load_state_dict(ckpt)\n",
    "#              print(f\"Autoencoder loaded successfully from {ckpt_path}\")\n",
    "#              autoenc.eval().requires_grad_(False).to(\"cpu\") # Keep on CPU initially\n",
    "#              utils.count_params(autoenc)\n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR loading autoencoder: {e}. Disabling blurry recon.\")\n",
    "#         blurry_recon = False # Turn off flag on error\n",
    "#         autoenc = None\n",
    "# else:\n",
    "#     print(\"Blurry reconstruction disabled by args, skipping Autoencoder load.\")\n",
    "\n",
    "# # Keep other models on CPU initially\n",
    "# clip_text_model.to(\"cpu\")\n",
    "# diffusion_engine.to(\"cpu\")\n",
    "\n",
    "# # Initialize result accumulators\n",
    "# all_blurryrecons = None\n",
    "# all_recons = None\n",
    "# all_predcaptions = [] # Use a standard list\n",
    "# all_clipvoxels = None\n",
    "\n",
    "# # Training Loop Settings\n",
    "# minibatch_size = 1\n",
    "# num_samples_per_image = 1\n",
    "# assert num_samples_per_image == 1\n",
    "# if utils.is_interactive(): plotting=False\n",
    "# dtype = torch.float16\n",
    "\n",
    "# # Check BrainNetwork internal flag\n",
    "# if hasattr(model, 'backbone') and hasattr(model.backbone, 'blurry_recon'):\n",
    "#      print(f\"Loaded model.backbone internal blurry_recon flag: {model.backbone.blurry_recon}\")\n",
    "# else: print(\"Warning: Cannot check internal blurry_recon flag.\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     test_images_idx_unique = np.unique(test_images_idx)\n",
    "#     test_images_idx_unique = test_images_idx_unique[:10] # Limit for debugging\n",
    "\n",
    "#     print(f\"Processing {len(test_images_idx_unique)} unique images...\")\n",
    "#     for batch_start in tqdm(range(0, len(test_images_idx_unique), minibatch_size)):\n",
    "#         batch_end = batch_start + minibatch_size\n",
    "#         uniq_imgs = test_images_idx_unique[batch_start:batch_end]\n",
    "\n",
    "#         # --- Voxel Preparation (CPU) ---\n",
    "#         voxel_list = []\n",
    "#         for uniq_img in uniq_imgs:\n",
    "#             locs = np.where(test_images_idx == uniq_img)[0]\n",
    "#             if len(locs) == 1: locs = np.repeat(locs, 3)\n",
    "#             elif len(locs) == 2: locs = np.concatenate([locs[[0]], locs])[:3]\n",
    "#             elif len(locs) > 3: locs = locs[:3]\n",
    "#             elif len(locs) < 1: continue\n",
    "#             assert len(locs) == 3\n",
    "#             max_voxel_idx = len(test_voxels) - 1\n",
    "#             valid_locs = [l for l in locs if l <= max_voxel_idx]\n",
    "#             if len(valid_locs) != 3: continue\n",
    "#             voxel_list.append(test_voxels[None, valid_locs])\n",
    "#         if not voxel_list: continue\n",
    "#         voxel = torch.cat(voxel_list, dim=0).to(\"cpu\")\n",
    "\n",
    "#         # --- MindEye Processing (CPU) ---\n",
    "#         all_backbone_reps = []\n",
    "#         all_clip_voxels_reps = []\n",
    "#         all_blurry_enc_reps = []\n",
    "#         for rep in range(3):\n",
    "#             voxel_ridge = model.ridge(voxel[:, [rep]], 0)\n",
    "#             backbone_out = model.backbone(voxel_ridge)\n",
    "#             # Robust Output Handling\n",
    "#             backbone_rep, clip_voxels_rep, blurry_latent_rep = None, None, None\n",
    "#             if isinstance(backbone_out, (list, tuple)) and len(backbone_out) >= 2:\n",
    "#                  backbone_rep = backbone_out[0]\n",
    "#                  clip_voxels_rep = backbone_out[1]\n",
    "#                  # --- Get Blurry Component Correctly ---\n",
    "#                  if len(backbone_out) >= 3:\n",
    "#                      blurry_component = backbone_out[2]\n",
    "#                      # Check if the component itself is the tensor or a tuple containing it\n",
    "#                      if isinstance(blurry_component, torch.Tensor):\n",
    "#                           blurry_latent_rep = blurry_component\n",
    "#                      elif isinstance(blurry_component, tuple) and len(blurry_component) > 0 and isinstance(blurry_component[0], torch.Tensor):\n",
    "#                           # Assuming the first element of the tuple is the desired latent\n",
    "#                           blurry_latent_rep = blurry_component[0]\n",
    "#                           # print(f\"[Debug] Extracted blurry latent from tuple (rep {rep})\") # Optional debug\n",
    "#                      else:\n",
    "#                           # Handle unexpected format or None case\n",
    "#                           blurry_latent_rep = None\n",
    "#                           # print(f\"[Debug] Blurry component was not tensor or valid tuple (rep {rep})\") # Optional debug\n",
    "#                  else:\n",
    "#                       blurry_latent_rep = None # No third element returned\n",
    "#             else:\n",
    "#                  print(\"Warning: Unexpected output structure from model.backbone.\")\n",
    "#                  backbone_rep = backbone_out\n",
    "#                  clip_voxels_rep = torch.zeros_like(backbone_rep) if isinstance(backbone_rep, torch.Tensor) else None\n",
    "#                  blurry_latent_rep = None\n",
    "#             # --- End Blurry Component Handling ---\n",
    "#             all_backbone_reps.append(backbone_rep)\n",
    "#             all_clip_voxels_reps.append(clip_voxels_rep)\n",
    "#             all_blurry_enc_reps.append(blurry_latent_rep)\n",
    "\n",
    "#         # Average - Check for None\n",
    "#         backbone = torch.mean(torch.stack([t.cpu() for t in all_backbone_reps if t is not None]), dim=0) if all(t is not None for t in all_backbone_reps) else None\n",
    "#         clip_voxels = torch.mean(torch.stack([t.cpu() for t in all_clip_voxels_reps if t is not None]), dim=0) if all(t is not None for t in all_clip_voxels_reps) else None\n",
    "#         blurry_image_enc = None\n",
    "#         if blurry_recon and all(isinstance(t, torch.Tensor) for t in all_blurry_enc_reps): # Check they are Tensors\n",
    "#             try:\n",
    "#                  blurry_tensors_for_stacking = [t.cpu() for t in all_blurry_enc_reps]\n",
    "#                  stacked_blurry_encs = torch.stack(blurry_tensors_for_stacking, dim=1) # Stack along dim 1 (reps)\n",
    "#                  blurry_image_enc = torch.mean(stacked_blurry_encs, dim=1) # Average over dim 1\n",
    "#                  print(f\"Averaged blurry latent shape: {blurry_image_enc.shape}\")\n",
    "#             except Exception as e: print(f\"Error averaging blurry latents: {e}. Setting to None.\")\n",
    "#         elif blurry_recon: print(\"Warning: Blurry latents not valid tensors.\")\n",
    "\n",
    "#         if backbone is None or clip_voxels is None: print(\"Skipping batch due to None backbone/clip_voxels.\"); continue\n",
    "\n",
    "#         # Store clipvoxels\n",
    "#         if all_clipvoxels is None: all_clipvoxels = clip_voxels.cpu()\n",
    "#         else: all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "\n",
    "#         # Diffusion Prior\n",
    "#         prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, text_cond=dict(text_embed=backbone), cond_scale=1., timesteps=20)\n",
    "\n",
    "#         # Caption Generation\n",
    "#         try:\n",
    "#             pred_caption_emb = clip_convert(prior_out)\n",
    "#             generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "#             generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#             all_predcaptions.extend(generated_caption) # Add to standard list\n",
    "#             print(f\"Batch {batch_start//minibatch_size}: {generated_caption}\")\n",
    "#         except Exception as caption_e:\n",
    "#             print(f\"Error during caption generation: {caption_e}\")\n",
    "#             all_predcaptions.extend([\"<caption_error>\"] * len(voxel))\n",
    "\n",
    "#         # --- unCLIP Reconstruction (Attempt on GPU) ---\n",
    "#         vector_suffix_gpu = None; batch_recons_tensor = None; ctx = None; samples = None\n",
    "#         reconstruction_succeeded = False\n",
    "#         try:\n",
    "#             print(\"Attempting reconstruction: Moving diffusion_engine to GPU...\")\n",
    "#             diffusion_engine.to(device)\n",
    "#             if hasattr(diffusion_engine.denoiser, 'sigmas') and diffusion_engine.denoiser.sigmas is not None:\n",
    "#                  diffusion_engine.denoiser.sigmas = diffusion_engine.denoiser.sigmas.to(device)\n",
    "#             current_batch_size = len(voxel)\n",
    "#             vs_repeated = vector_suffix.repeat(current_batch_size, 1) if vector_suffix.shape[0] != current_batch_size else vector_suffix\n",
    "#             vector_suffix_gpu = vs_repeated.to(device)\n",
    "#             print(\"Generating reconstructions...\")\n",
    "#             batch_recons = []\n",
    "#             with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "#                  for i in range(len(voxel)):\n",
    "#                      ctx = F.pad(prior_out[[i]], (0, 1664 - clip_emb_dim)).to(device)\n",
    "#                      # Make sure utils.unclip_recon exists and is imported\n",
    "#                      samples = utils.unclip_recon(ctx, diffusion_engine, vector_suffix_gpu[[i]], num_samples=num_samples_per_image)\n",
    "#                      batch_recons.append(samples.cpu())\n",
    "#                      del ctx; del samples\n",
    "#             if batch_recons:\n",
    "#                  batch_recons_tensor = torch.cat(batch_recons, dim=0)\n",
    "#                  if all_recons is None: all_recons = batch_recons_tensor\n",
    "#                  else: all_recons = torch.vstack((all_recons, batch_recons_tensor))\n",
    "#                  reconstruction_succeeded = True\n",
    "#             del batch_recons;\n",
    "#             if batch_recons_tensor is not None: del batch_recons_tensor\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during reconstruction: {e}\")\n",
    "#             if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory ---\\n\")\n",
    "#             reconstruction_succeeded = False\n",
    "#         finally:\n",
    "#             print(\"Moving diffusion_engine back to CPU...\")\n",
    "#             diffusion_engine.to(\"cpu\")\n",
    "#             if vector_suffix_gpu is not None: del vector_suffix_gpu\n",
    "#             torch.cuda.empty_cache()\n",
    "#         if reconstruction_succeeded: print(\"Reconstruction successful.\")\n",
    "\n",
    "#         # --- Blurry Reconstruction (Attempt on GPU IF ENABLED and Latent Available) ---\n",
    "#         if blurry_recon and autoenc is not None and blurry_image_enc is not None:\n",
    "#              blurry_image_enc_gpu = None; blurred_image_gpu = None; blurred_image = None\n",
    "#              try:\n",
    "#                 print(\"Attempting blurry reconstruction: Moving autoenc to GPU...\")\n",
    "#                 autoenc.to(device)\n",
    "#                 blurry_image_enc_gpu = blurry_image_enc.to(device)\n",
    "#                 print(f\"Shape feeding into blurry decode: {blurry_image_enc_gpu.shape}\")\n",
    "\n",
    "#                 print(\"Decoding blurry images...\")\n",
    "#                 with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "#                      # VAE Scaling: Divide by scale factor before decode, then un-normalize\n",
    "#                      # Ensure autoenc.config.scaling_factor is accessible and correct\n",
    "#                      scale_factor_vae = getattr(getattr(autoenc, 'config', None), 'scaling_factor', 0.18215) # Use default if not found\n",
    "#                      blurred_image_gpu = (autoenc.decode(blurry_image_enc_gpu / scale_factor_vae).sample / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "#                 blurred_image = blurred_image_gpu.cpu()\n",
    "#                 print(\"Blurry decoding done, images moved to CPU.\")\n",
    "\n",
    "#                 # Aggregate blurry recons\n",
    "#                 if all_blurryrecons is None: all_blurryrecons = blurred_image\n",
    "#                 else: all_blurryrecons = torch.vstack((all_blurryrecons, blurred_image))\n",
    "\n",
    "#              except Exception as e:\n",
    "#                  print(f\"Error during blurry reconstruction: {e}\")\n",
    "#                  if 'Expected' in str(e) and 'input' in str(e): print(\">>> Shape mismatch feeding into autoenc.decode.\")\n",
    "#                  if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory during Blurry Recon ---\\n\")\n",
    "#              finally:\n",
    "#                 print(\"Moving autoenc back to CPU...\")\n",
    "#                 autoenc.to(\"cpu\")\n",
    "#                 if blurry_image_enc_gpu is not None: del blurry_image_enc_gpu\n",
    "#                 if blurred_image_gpu is not None: del blurred_image_gpu\n",
    "#                 torch.cuda.empty_cache()\n",
    "#         elif blurry_recon:\n",
    "#              print(\"Skipping blurry reconstruction for this batch (autoenc or latent missing/invalid).\")\n",
    "\n",
    "#         # End of loop cleanup\n",
    "#         del voxel, voxel_list, voxel_ridge, backbone, clip_voxels, blurry_image_enc\n",
    "#         del all_backbone_reps, all_clip_voxels_reps, all_blurry_enc_reps\n",
    "#         del prior_out\n",
    "\n",
    "# # --- Post-processing and Saving ---\n",
    "# imsize = 256\n",
    "# if all_recons is not None and len(all_recons) > 0:\n",
    "#     print(f\"Resizing {len(all_recons)} reconstructions to {imsize}x{imsize}...\")\n",
    "#     all_recons = transforms.Resize((imsize, imsize), antialias=True)(all_recons.float())\n",
    "# else: all_recons = None\n",
    "\n",
    "# if blurry_recon and all_blurryrecons is not None and len(all_blurryrecons) > 0:\n",
    "#     print(f\"Resizing {len(all_blurryrecons)} blurry reconstructions to {imsize}x{imsize}...\")\n",
    "#     all_blurryrecons = transforms.Resize((imsize, imsize), antialias=True)(all_blurryrecons.float())\n",
    "# else: all_blurryrecons = None\n",
    "\n",
    "# # Saving\n",
    "# print(\"Saving outputs...\")\n",
    "# output_dir = f\"evals/{model_name}\"\n",
    "# # --- Add Debugging Prints for Saving ---\n",
    "# print(f\"[Debug] Output directory variable: {output_dir}\")\n",
    "# print(f\"[Debug] Absolute output directory: {os.path.abspath(output_dir)}\")\n",
    "# print(f\"[Debug] Attempting to create directory: {os.path.abspath(output_dir)}\")\n",
    "# try:\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     print(f\"[Debug] Directory exists after makedirs: {os.path.isdir(output_dir)}\")\n",
    "#     if hasattr(os, 'access'): print(f\"[Debug] Write permission for {output_dir}: {os.access(output_dir, os.W_OK)}\")\n",
    "# except Exception as e: print(f\"[Debug] ERROR creating directory {output_dir}: {e}\")\n",
    "# # --- End Debugging ---\n",
    "\n",
    "# if all_recons is not None:\n",
    "#     recon_path = os.path.join(output_dir, f\"{model_name}_all_recons.pt\")\n",
    "#     print(f\"Shape of saved recons: {all_recons.shape}. Saving to {recon_path}...\")\n",
    "#     print(f\"[Debug] Absolute recon path: {os.path.abspath(recon_path)}\")\n",
    "#     print(f\"[Debug] Directory for recon path exists: {os.path.isdir(os.path.dirname(recon_path))}\")\n",
    "#     try:\n",
    "#         torch.save(all_recons, recon_path)\n",
    "#         print(\"Recons saved successfully.\")\n",
    "#     except Exception as e: print(f\"!!! ERROR saving recons to {recon_path}: {e}\")\n",
    "# else: print(\"Skipping saving of image reconstructions (all_recons is None).\")\n",
    "\n",
    "# if all_blurryrecons is not None:\n",
    "#      blurry_path = os.path.join(output_dir, f\"{model_name}_all_blurryrecons.pt\")\n",
    "#      print(f\"Shape of saved blurry recons: {all_blurryrecons.shape}. Saving to {blurry_path}...\")\n",
    "#      try:\n",
    "#          torch.save(all_blurryrecons, blurry_path)\n",
    "#          print(\"Blurry recons saved successfully.\")\n",
    "#      except Exception as e: print(f\"!!! ERROR saving blurry recons to {blurry_path}: {e}\")\n",
    "# else: print(\"Skipping saving of blurry reconstructions.\")\n",
    "\n",
    "# # --- Corrected Caption Saving ---\n",
    "# if all_predcaptions:\n",
    "#      if not isinstance(all_predcaptions, list) or not all(isinstance(item, str) for item in all_predcaptions):\n",
    "#           print(\"Warning: all_predcaptions not list of strings. Converting...\")\n",
    "#           try: all_predcaptions = [str(item) for item in all_predcaptions]\n",
    "#           except Exception as conv_e: print(f\"Error converting captions: {conv_e}. Skipping save.\"); all_predcaptions = None\n",
    "#      if all_predcaptions:\n",
    "#           caption_path = os.path.join(output_dir, f\"{model_name}_all_predcaptions.pt\")\n",
    "#           print(f\"Saving {len(all_predcaptions)} predicted captions to {caption_path}...\")\n",
    "#           try:\n",
    "#                torch.save(all_predcaptions, caption_path)\n",
    "#                print(\"Captions saved successfully.\")\n",
    "#           except Exception as save_e:\n",
    "#                print(f\"!!! Error saving captions with torch.save: {save_e}\")\n",
    "#                print(\"Attempting fallback to .npy...\")\n",
    "#                try:\n",
    "#                     caption_path_npy = os.path.join(output_dir, f\"{model_name}_all_predcaptions.npy\")\n",
    "#                     np.save(caption_path_npy, np.array(all_predcaptions, dtype=object))\n",
    "#                     print(f\"Captions saved as fallback to {caption_path_npy}\")\n",
    "#                except Exception as npy_save_e: print(f\"!!! Fallback saving as .npy also failed: {npy_save_e}\")\n",
    "# else: print(\"Skipping saving of captions.\")\n",
    "# # -------------------------------\n",
    "\n",
    "# if all_clipvoxels is not None:\n",
    "#     clipvoxels_path = os.path.join(output_dir, f\"{model_name}_all_clipvoxels.pt\")\n",
    "#     print(f\"Shape of saved clipvoxels: {all_clipvoxels.shape}. Saving to {clipvoxels_path}...\")\n",
    "#     try:\n",
    "#         torch.save(all_clipvoxels, clipvoxels_path)\n",
    "#         print(\"Clipvoxels saved successfully.\")\n",
    "#     except Exception as e: print(f\"!!! ERROR saving clipvoxels to {clipvoxels_path}: {e}\")\n",
    "# else: print(\"Skipping saving of clipvoxels (all_clipvoxels is None).\")\n",
    "\n",
    "\n",
    "# print(f\"Saved outputs for {model_name} in {output_dir}\")\n",
    "\n",
    "# if not utils.is_interactive():\n",
    "#     sys.exit(0)\n",
    "\n",
    "# # %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a85893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available conditioner embedders:\n",
      "  Index: 0, Type: <class 'sgm.modules.encoders.modules.FrozenOpenCLIPImageEmbedder'>\n",
      "  Index: 1, Type: <class 'sgm.modules.encoders.modules.ConcatTimestepEmbedderND'>\n",
      "  Index: 2, Type: <class 'sgm.modules.encoders.modules.ConcatTimestepEmbedderND'>\n",
      "Using vector_suffix_shape: (1, 1024)\n",
      "Created placeholder vector_suffix with shape: torch.Size([1, 1024]). Needs verification!\n",
      "Attempting to load Autoencoder for Blurry Recon (Training Loss Component)...\n",
      "Autoencoder loaded successfully from /workspace/MindEyeV2/MindEyeV2/src/cache/sd_image_var_autoenc.pth\n",
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "Loaded model.backbone internal blurry_recon flag: True\n",
      "Processing 10 unique images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Blurry latents not valid tensors from all reps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798480c71a5d47e393fb2c59c641df95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving clip_text_model to GPU (FP16) for caption generation...\n",
      "Error during caption generation for batch 0: output with shape [1, 1, 1, 1] doesn't match the broadcast shape [1, 1, 1, 2]\n",
      "Clearing cache before reconstruction step...\n",
      "Converting diffusion_engine to FP16 (half precision)...\n",
      "Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\n",
      "Generating reconstructions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during reconstruction: Input type (float) and bias type (c10::Half) should be the same\n",
      "Moving diffusion_engine back to CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:17<02:40, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction failed or was skipped.\n",
      "Warning: Blurry latents not valid tensors from all reps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74669fa63d9d4dcc8fbabe96401af11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving clip_text_model to GPU (FP16) for caption generation...\n",
      "Error during caption generation for batch 1: output with shape [1, 1, 1, 1] doesn't match the broadcast shape [1, 1, 1, 2]\n",
      "Clearing cache before reconstruction step...\n",
      "Converting diffusion_engine to FP16 (half precision)...\n",
      "Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\n",
      "Generating reconstructions...\n",
      "Error during reconstruction: Input type (float) and bias type (c10::Half) should be the same\n",
      "Moving diffusion_engine back to CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:33<02:11, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction failed or was skipped.\n",
      "Warning: Blurry latents not valid tensors from all reps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0228806990c7492ab2b6645f3c9ea507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving clip_text_model to GPU (FP16) for caption generation...\n",
      "Error during caption generation for batch 2: output with shape [1, 1, 1, 1] doesn't match the broadcast shape [1, 1, 1, 2]\n",
      "Clearing cache before reconstruction step...\n",
      "Converting diffusion_engine to FP16 (half precision)...\n",
      "Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\n",
      "Generating reconstructions...\n",
      "Error during reconstruction: Input type (float) and bias type (c10::Half) should be the same\n",
      "Moving diffusion_engine back to CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:48<01:52, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction failed or was skipped.\n",
      "Warning: Blurry latents not valid tensors from all reps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c0ffbe58044b3e8d78fa6594dccb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving clip_text_model to GPU (FP16) for caption generation...\n",
      "Error during caption generation for batch 3: output with shape [1, 1, 1, 1] doesn't match the broadcast shape [1, 1, 1, 2]\n",
      "Clearing cache before reconstruction step...\n",
      "Converting diffusion_engine to FP16 (half precision)...\n",
      "Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\n",
      "Generating reconstructions...\n",
      "Error during reconstruction: Input type (float) and bias type (c10::Half) should be the same\n",
      "Moving diffusion_engine back to CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:04<01:34, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction failed or was skipped.\n",
      "Warning: Blurry latents not valid tensors from all reps.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00584d796564d38bdfced77f665b329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving clip_text_model to GPU (FP16) for caption generation...\n",
      "Error during caption generation for batch 4: output with shape [1, 1, 1, 1] doesn't match the broadcast shape [1, 1, 1, 2]\n",
      "Clearing cache before reconstruction step...\n",
      "Converting diffusion_engine to FP16 (half precision)...\n",
      "Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\n",
      "Generating reconstructions...\n",
      "Moving diffusion_engine back to CPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:10<01:46, 17.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 295\u001b[0m\n\u001b[1;32m    292\u001b[0m ctx \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(prior_out[[i]]\u001b[38;5;241m.\u001b[39mcpu(), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1664\u001b[39m \u001b[38;5;241m-\u001b[39m prior_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m    293\u001b[0m current_vector_suffix \u001b[38;5;241m=\u001b[39m vector_suffix_gpu[[i]]\n\u001b[0;32m--> 295\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munclip_recon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_vector_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples_per_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m batch_recons\u001b[38;5;241m.\u001b[39mappend(samples\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcpu()) \u001b[38;5;66;03m# Convert back to FP32 on CPU\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ctx; \u001b[38;5;28;01mdel\u001b[39;00m samples; \u001b[38;5;28;01mdel\u001b[39;00m current_vector_suffix\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/utils.py:303\u001b[0m, in \u001b[0;36munclip_recon\u001b[0;34m(x, diffusion_engine, vector_suffix, num_samples, offset_noise_level)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdenoiser\u001b[39m(x, sigma, c):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m diffusion_engine\u001b[38;5;241m.\u001b[39mdenoiser(diffusion_engine\u001b[38;5;241m.\u001b[39mmodel, x, sigma, c)\n\u001b[0;32m--> 303\u001b[0m samples_z \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoised_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m samples_x \u001b[38;5;241m=\u001b[39m diffusion_engine\u001b[38;5;241m.\u001b[39mdecode_first_stage(samples_z)\n\u001b[1;32m    305\u001b[0m samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp((samples_x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.8\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m.2\u001b[39m), \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/sampling.py:120\u001b[0m, in \u001b[0;36mEDMSampler.__call__\u001b[0;34m(self, denoiser, x, cond, uc, num_steps)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sigma_gen(num_sigmas):\n\u001b[1;32m    115\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_churn \u001b[38;5;241m/\u001b[39m (num_sigmas \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_tmin \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sigmas[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_tmax\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/sampling.py:99\u001b[0m, in \u001b[0;36mEDMSampler.sampler_step\u001b[0;34m(self, sigma, next_sigma, denoiser, x, cond, uc, gamma)\u001b[0m\n\u001b[1;32m     96\u001b[0m     eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_noise\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m append_dims(sigma_hat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m sigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39mndim) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 99\u001b[0m denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m d \u001b[38;5;241m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[1;32m    101\u001b[0m dt \u001b[38;5;241m=\u001b[39m append_dims(next_sigma \u001b[38;5;241m-\u001b[39m sigma_hat, x\u001b[38;5;241m.\u001b[39mndim)\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/sampling.py:55\u001b[0m, in \u001b[0;36mBaseDiffusionSampler.denoise\u001b[0;34m(self, x, denoiser, sigma, cond, uc)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdenoise\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, denoiser, sigma, cond, uc):\n\u001b[0;32m---> 55\u001b[0m     denoised \u001b[38;5;241m=\u001b[39m \u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguider(denoised, sigma)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m denoised\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/utils.py:301\u001b[0m, in \u001b[0;36munclip_recon.<locals>.denoiser\u001b[0;34m(x, sigma, c)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdenoiser\u001b[39m(x, sigma, c):\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdiffusion_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/denoiser.py:37\u001b[0m, in \u001b[0;36mDenoiser.forward\u001b[0;34m(self, network, input, sigma, cond, **additional_model_inputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m c_skip, c_out, c_in, c_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling(sigma)\n\u001b[1;32m     35\u001b[0m c_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossibly_quantize_c_noise(c_noise\u001b[38;5;241m.\u001b[39mreshape(sigma_shape))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_model_inputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m c_out\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m c_skip\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/wrappers.py:28\u001b[0m, in \u001b[0;36mOpenAIWrapper.forward\u001b[0;34m(self, x, t, c, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor, c: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, c\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor([])\u001b[38;5;241m.\u001b[39mtype_as(x))), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrossattn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/openaimodel.py:850\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_blocks:\n\u001b[1;32m    849\u001b[0m     h \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([h, hs\u001b[38;5;241m.\u001b[39mpop()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 850\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(h)\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/diffusionmodules/openaimodel.py:101\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context, image_only_indicator, time_context, num_video_frames)\u001b[0m\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m     94\u001b[0m         x,\n\u001b[1;32m     95\u001b[0m         context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         image_only_indicator,\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, SpatialTransformer):\n\u001b[0;32m--> 101\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/attention.py:717\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(context) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    716\u001b[0m         i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# use same context for each block\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear:\n\u001b[1;32m    719\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/attention.py:546\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context, additional_tokens, n_times_crossframe_attn_in_self)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# return mixed_checkpoint(self._forward, kwargs, self.parameters(), self.checkpoint)\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;66;03m# inputs = {\"x\": x, \"context\": context}\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# return checkpoint(self._forward, inputs, self.parameters(), self.checkpoint)\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/workspace/MindEyeV2/MindEyeV2/src/generative_models/sgm/modules/attention.py:556\u001b[0m, in \u001b[0;36mBasicTransformerBlock._forward\u001b[0;34m(self, x, context, additional_tokens, n_times_crossframe_attn_in_self)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward\u001b[39m(\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, additional_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_times_crossframe_attn_in_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    553\u001b[0m ):\n\u001b[1;32m    554\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m--> 556\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    557\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_self_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             additional_tokens\u001b[38;5;241m=\u001b[39madditional_tokens,\n\u001b[1;32m    559\u001b[0m             n_times_crossframe_attn_in_self\u001b[38;5;241m=\u001b[39mn_times_crossframe_attn_in_self\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_self_attn\n\u001b[1;32m    561\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    562\u001b[0m         )\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    564\u001b[0m     )\n\u001b[1;32m    565\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\n\u001b[1;32m    567\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), context\u001b[38;5;241m=\u001b[39mcontext, additional_tokens\u001b[38;5;241m=\u001b[39madditional_tokens\n\u001b[1;32m    568\u001b[0m         )\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    571\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# %%\n",
    "# recon_inference.ipynb - Main processing cell (REVISED AGAIN)\n",
    "\n",
    "import inspect\n",
    "import os # Make sure os is imported if not already\n",
    "import torch # Make sure torch is imported\n",
    "import numpy as np # Make sure numpy is imported if not already\n",
    "import torch.nn.functional as F # Import functional if needed by utils.unclip_recon\n",
    "from tqdm import tqdm # Ensure tqdm is imported\n",
    "from torchvision import transforms # Ensure transforms are imported\n",
    "\n",
    "# --- Ensure blurry_recon argument is used ---\n",
    "\n",
    "# (vector_suffix inference logic - keep the corrected version)\n",
    "try:\n",
    "    embed_dim = None; embedder_info = []\n",
    "    if hasattr(diffusion_engine, 'conditioner') and hasattr(diffusion_engine.conditioner, 'embedders'):\n",
    "        print(\"Available conditioner embedders:\")\n",
    "        # Correctly iterate over ModuleList\n",
    "        for i, emb in enumerate(diffusion_engine.conditioner.embedders): # Use enumerate for ModuleList\n",
    "            emb_type = type(emb); embedder_info.append(f\"  Index: {i}, Type: {emb_type}\")\n",
    "            # Simplified checks for common types\n",
    "            if isinstance(emb, FrozenOpenCLIPEmbedder2):\n",
    "                 # Safely access attributes\n",
    "                 proj_layer = getattr(getattr(emb, 'model', None), 'text_projection', None)\n",
    "                 if proj_layer is not None and hasattr(proj_layer, 'shape'):\n",
    "                      embed_dim = proj_layer.shape[-1]\n",
    "                      embedder_info[-1] += f\" -> Found embed_dim: {embed_dim} (Preferred)\"\n",
    "                      break # Found preferred, stop checking\n",
    "                 else:\n",
    "                      embedder_info[-1] += \" -> Warning: Could not get text_projection shape\"\n",
    "            elif isinstance(emb, FrozenOpenCLIPImageEmbedder):\n",
    "                 found_dim = getattr(emb, 'output_dim', getattr(getattr(emb, 'model', None), 'output_dim', None))\n",
    "                 if found_dim and embed_dim is None: # Use if no preferred found yet\n",
    "                     embed_dim = found_dim\n",
    "                     embedder_info[-1] += f\" -> Found embed_dim: {found_dim}\"\n",
    "    for info in embedder_info: print(info)\n",
    "    # Ensure embed_dim is an integer before using it for shape\n",
    "    vector_suffix_shape = (1, embed_dim) if isinstance(embed_dim, int) else (1, 1024)\n",
    "    print(f\"Using vector_suffix_shape: {vector_suffix_shape}\")\n",
    "    vector_suffix = torch.zeros(vector_suffix_shape, device='cpu', dtype=torch.float32)\n",
    "    print(f\"Created placeholder vector_suffix with shape: {vector_suffix.shape}. Needs verification!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inferring vector_suffix shape: {e}. Using default (1, 1024).\")\n",
    "    vector_suffix = torch.zeros(1, 1024, device='cpu', dtype=torch.float32)\n",
    "\n",
    "# %%\n",
    "# get all reconstructions\n",
    "model.to(\"cpu\")\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# --- Load Autoencoder Conditionally ---\n",
    "autoenc = None # Ensure initialized to None\n",
    "# --- Keep Autoencoder loading if blurry_recon is ON, needed for training loss, but not for inference decoding ---\n",
    "if blurry_recon:\n",
    "    print(\"Attempting to load Autoencoder for Blurry Recon (Training Loss Component)...\")\n",
    "    try:\n",
    "        from diffusers import AutoencoderKL\n",
    "        autoenc = AutoencoderKL(\n",
    "             down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "             up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "             block_out_channels=[128, 256, 512, 512], layers_per_block=2,\n",
    "             # --- CORRECTED: Use sample_size 256 as per typical SD VAE checkpoints ---\n",
    "             sample_size=256,\n",
    "         )\n",
    "        ckpt_path = f'{cache_dir}/sd_image_var_autoenc.pth'\n",
    "        if not os.path.exists(ckpt_path):\n",
    "             print(f\"ERROR: Autoencoder checkpoint file not found at {ckpt_path}. Blurry recon components might be affected.\")\n",
    "             # We don't disable blurry_recon here as the backbone might still output the latent for other uses\n",
    "             autoenc = None # Set to None if file missing\n",
    "        else:\n",
    "             ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "             autoenc.load_state_dict(ckpt)\n",
    "             print(f\"Autoencoder loaded successfully from {ckpt_path}\")\n",
    "             autoenc.eval().requires_grad_(False).to(\"cpu\") # Keep on CPU initially\n",
    "             utils.count_params(autoenc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading autoencoder: {e}. Blurry recon components might be affected.\")\n",
    "        autoenc = None # Set to None on error\n",
    "else:\n",
    "    print(\"Blurry reconstruction disabled by args, skipping Autoencoder load.\")\n",
    "\n",
    "\n",
    "# Keep other models on CPU initially\n",
    "clip_text_model.to(\"cpu\")\n",
    "diffusion_engine.to(\"cpu\")\n",
    "\n",
    "# Initialize result accumulators\n",
    "# all_blurryrecons = None # --- Comment out or remove if not decoding blurry latent ---\n",
    "all_recons = None\n",
    "all_predcaptions = [] # Use a standard list\n",
    "all_clipvoxels = None\n",
    "all_backbones = None # Add accumulator for backbone output if needed for UMAP\n",
    "all_prior_out = None # Add accumulator for prior output if needed for UMAP\n",
    "\n",
    "\n",
    "# Training Loop Settings\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "assert num_samples_per_image == 1\n",
    "if utils.is_interactive(): plotting=False\n",
    "# --- CORRECTED: Use FP16 for GPU operations ---\n",
    "dtype = torch.float16\n",
    "\n",
    "# Check BrainNetwork internal flag\n",
    "if hasattr(model, 'backbone') and hasattr(model.backbone, 'blurry_recon'):\n",
    "     print(f\"Loaded model.backbone internal blurry_recon flag: {model.backbone.blurry_recon}\")\n",
    "else: print(\"Warning: Cannot check internal blurry_recon flag.\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_images_idx_unique = np.unique(test_images_idx)\n",
    "    test_images_idx_unique = test_images_idx_unique[:10] # Limit for debugging\n",
    "\n",
    "    print(f\"Processing {len(test_images_idx_unique)} unique images...\")\n",
    "    for batch_start in tqdm(range(0, len(test_images_idx_unique), minibatch_size)):\n",
    "        batch_end = batch_start + minibatch_size\n",
    "        uniq_imgs = test_images_idx_unique[batch_start:batch_end]\n",
    "\n",
    "        # --- Voxel Preparation (CPU) ---\n",
    "        voxel_list = []\n",
    "        for uniq_img in uniq_imgs:\n",
    "            locs = np.where(test_images_idx == uniq_img)[0]\n",
    "            # --- Ensure you handle cases with fewer than 3 repetitions if needed ---\n",
    "            if len(locs) < 1: continue # Skip images with no fMRI data\n",
    "            # Pad/repeat locs to always have 3 if the model expects it\n",
    "            if len(locs) == 1: locs = np.repeat(locs, 3)\n",
    "            elif len(locs) == 2: locs = np.concatenate([locs, locs[:1]]) # Repeat first element\n",
    "            elif len(locs) > 3: locs = locs[:3]\n",
    "            # Ensure valid indices before accessing test_voxels\n",
    "            max_voxel_sample_idx = len(test_voxels) - 1\n",
    "            valid_locs = [l for l in locs if l >= 0 and l <= max_voxel_sample_idx]\n",
    "            if len(valid_locs) != 3: # After ensuring 3 locs, re-check validity\n",
    "                 print(f\"Warning: Skipping unique image {uniq_img} due to invalid or insufficient voxel sample indices ({len(valid_locs)} found).\")\n",
    "                 continue # Skip batch if valid locs are not 3\n",
    "\n",
    "            voxel_list.append(test_voxels[None, valid_locs])\n",
    "\n",
    "        if not voxel_list: continue # Skip batch if no valid images were found after checks\n",
    "        voxel = torch.cat(voxel_list, dim=0).to(\"cpu\")\n",
    "\n",
    "        # --- MindEye Processing (CPU) ---\n",
    "        all_backbone_reps = []\n",
    "        all_clip_voxels_reps = []\n",
    "        # --- FIX: Uncomment this line ---\n",
    "        all_blurry_enc_reps = [] # Accumulator for blurry latent if needed\n",
    "        for rep in range(3):\n",
    "            voxel_ridge = model.ridge(voxel[:, [rep]], 0)\n",
    "            backbone_out = model.backbone(voxel_ridge)\n",
    "\n",
    "            # Robust Output Handling based on BrainNetwork's return (backbone, c, b)\n",
    "            backbone_rep, clip_voxels_rep, blurry_latent_rep = None, None, None # Initialize blurry_latent_rep\n",
    "            if isinstance(backbone_out, (list, tuple)) and len(backbone_out) >= 2:\n",
    "                 backbone_rep = backbone_out[0]\n",
    "                 clip_voxels_rep = backbone_out[1]\n",
    "                 if len(backbone_out) >= 3:\n",
    "                     blurry_component = backbone_out[2]\n",
    "                     # --- FIX: Uncomment and adjust blurry latent extraction if needed ---\n",
    "                     # Check if the component itself is the tensor or a tuple containing it\n",
    "                     if isinstance(blurry_component, torch.Tensor):\n",
    "                          blurry_latent_rep = blurry_component\n",
    "                     elif isinstance(blurry_component, tuple) and len(blurry_component) > 0 and isinstance(blurry_component[0], torch.Tensor):\n",
    "                          # Assuming the first element of the tuple is the desired latent\n",
    "                          blurry_latent_rep = blurry_component[0] \n",
    "                     else:\n",
    "                          blurry_latent_rep = None # Handle unexpected format or None case\n",
    "                 else:\n",
    "                      blurry_latent_rep = None # No third element returned\n",
    "            else:\n",
    "                 print(\"Warning: Unexpected output structure from model.backbone.\")\n",
    "                 backbone_rep = backbone_out\n",
    "                 clip_voxels_rep = torch.zeros_like(backbone_rep) if isinstance(backbone_rep, torch.Tensor) else None\n",
    "                 # blurry_latent_rep = None # Cannot get blurry latent if output structure is wrong\n",
    "\n",
    "            all_backbone_reps.append(backbone_rep)\n",
    "            all_clip_voxels_reps.append(clip_voxels_rep)\n",
    "            # if blurry_latent_rep is not None: all_blurry_enc_reps.append(blurry_latent_rep)\n",
    "\n",
    "\n",
    "        # Average - Check for None before stacking/averaging\n",
    "        # Ensure all reps produced valid tensors before averaging\n",
    "        if not all(t is not None for t in all_backbone_reps) or not all(t is not None for t in all_clip_voxels_reps):\n",
    "             print(f\"Skipping batch {batch_start//minibatch_size} due to None backbone/clip_voxels outputs from reps.\")\n",
    "             continue # Skip this batch\n",
    "\n",
    "        backbone = torch.mean(torch.stack([t.cpu() for t in all_backbone_reps]), dim=0)\n",
    "        clip_voxels = torch.mean(torch.stack([t.cpu() for t in all_clip_voxels_reps]), dim=0)\n",
    "\n",
    "        # Average blurry latent if needed (and if the code path to collect it above is uncommented)\n",
    "        blurry_image_enc = None\n",
    "        if blurry_recon and all_blurry_enc_reps and all(isinstance(t, torch.Tensor) for t in all_blurry_enc_reps):\n",
    "            try:\n",
    "                 # Stack along dim 1 (reps), then average\n",
    "                 stacked_blurry_encs = torch.stack([t.cpu() for t in all_blurry_enc_reps], dim=1)\n",
    "                 blurry_image_enc = torch.mean(stacked_blurry_encs, dim=1)\n",
    "                 print(f\"Averaged blurry latent shape: {blurry_image_enc.shape}\")\n",
    "            except Exception as e: print(f\"Error averaging blurry latents: {e}. Setting to None.\")\n",
    "        elif blurry_recon: print(\"Warning: Blurry latents not valid tensors from all reps.\")\n",
    "\n",
    "\n",
    "        # Store backbone, clipvoxels, prior_out if needed for UMAP/evals\n",
    "        if all_backbones is None: all_backbones = backbone.cpu()\n",
    "        else: all_backbones = torch.vstack((all_backbones, backbone.cpu()))\n",
    "        if all_clipvoxels is None: all_clipvoxels = clip_voxels.cpu()\n",
    "        else: all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "\n",
    "        # Diffusion Prior (on CPU initially, result is prior_out on CPU)\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, text_cond=dict(text_embed=backbone), cond_scale=1., timesteps=20)\n",
    "\n",
    "        # Store prior_out if needed for UMAP/evals\n",
    "        if all_prior_out is None: all_prior_out = prior_out.cpu()\n",
    "        else: all_prior_out = torch.vstack((all_prior_out, prior_out.cpu()))\n",
    "\n",
    "        # --- Cleanup before Captioning ---\n",
    "        del backbone, clip_voxels # Delete averaged tensors now they are stored/used\n",
    "        del all_backbone_reps, all_clip_voxels_reps # Delete lists\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Caption Generation\n",
    "        generated_caption = [\"<caption_error>\"] * len(voxel) # Default value\n",
    "        try:\n",
    "            # Ensure clip_convert is on CPU\n",
    "            clip_convert.to(\"cpu\")\n",
    "            # Convert on CPU\n",
    "            pred_caption_emb_cpu = clip_convert(prior_out.cpu())\n",
    "\n",
    "            # Move model to GPU, try FP16 this time as FP32 didn't help shape mismatch\n",
    "            print(\"Moving clip_text_model to GPU (FP16) for caption generation...\")\n",
    "            clip_text_model.half().to(device) # Use FP16\n",
    "\n",
    "            # Ensure input tensor is contiguous AND FP16 on the correct device\n",
    "            pixel_values_input = pred_caption_emb_cpu.half().to(device).contiguous()\n",
    "\n",
    "            # Generate captions - Use simple greedy search (num_beams=1) to minimize complexity\n",
    "            # Use autocast context\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16, enabled=True):\n",
    "                 generated_ids = clip_text_model.generate(\n",
    "                     pixel_values=pixel_values_input,\n",
    "                     max_length=20,\n",
    "                     num_beams=1, # Use greedy search\n",
    "                     do_sample=False # Disable sampling\n",
    "                 )\n",
    "\n",
    "            # Move model back to CPU immediately\n",
    "            clip_text_model.to(\"cpu\").float() # Move to CPU and restore FP32\n",
    "\n",
    "            # Process results on CPU\n",
    "            generated_caption = processor.batch_decode(generated_ids.cpu(), skip_special_tokens=True)\n",
    "            print(f\"Batch {batch_start//minibatch_size}: {generated_caption}\")\n",
    "\n",
    "            # --- Cleanup ---\n",
    "            del pixel_values_input, generated_ids, pred_caption_emb_cpu\n",
    "\n",
    "        except Exception as caption_e:\n",
    "            print(f\"Error during caption generation for batch {batch_start//minibatch_size}: {caption_e}\")\n",
    "            # generated_caption remains [\"<caption_error>\"]\n",
    "        finally:\n",
    "            # Ensure model is back on CPU and FP32 even if there's an error\n",
    "            clip_text_model.to(\"cpu\").float()\n",
    "            torch.cuda.empty_cache() # Clear cache after captioning\n",
    "\n",
    "        all_predcaptions.extend(generated_caption) # Extend with result or error placeholder\n",
    "\n",
    "        # --- Explicit Cleanup Before Reconstruction ---\n",
    "        print(\"Clearing cache before reconstruction step...\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- unCLIP Reconstruction (Attempt on GPU) ---\n",
    "        vector_suffix_gpu = None; batch_recons_tensor = None; ctx = None; samples = None\n",
    "        reconstruction_succeeded = False\n",
    "        try:\n",
    "            # --- FIX: Explicitly use FP16 for diffusion engine BEFORE moving ---\n",
    "            print(\"Converting diffusion_engine to FP16 (half precision)...\")\n",
    "            diffusion_engine.half() # Use .half() to convert model parameters to FP16\n",
    "\n",
    "            print(\"Attempting reconstruction: Moving diffusion_engine (FP16) to GPU...\")\n",
    "            diffusion_engine.to(device) # Move the FP16 model to GPU\n",
    "\n",
    "            # Make sure diffusion engine components are on device if they have state (like sigmas)\n",
    "            if hasattr(diffusion_engine.denoiser, 'sigmas') and diffusion_engine.denoiser.sigmas is not None:\n",
    "                 diffusion_engine.denoiser.sigmas = diffusion_engine.denoiser.sigmas.to(device=device, dtype=torch.float16)\n",
    "\n",
    "            current_batch_size = len(voxel)\n",
    "            vs_repeated = vector_suffix.float().repeat(current_batch_size, 1) if vector_suffix.shape[0] != current_batch_size else vector_suffix.float()\n",
    "            vector_suffix_gpu = vs_repeated.to(device).half() # Move to GPU and convert to FP16\n",
    "\n",
    "            print(\"Generating reconstructions...\")\n",
    "            batch_recons = []\n",
    "            # Use autocast with dtype=torch.float16\n",
    "            with torch.cuda.amp.autocast(dtype=dtype, enabled=True): # dtype is torch.float16\n",
    "                 for i in range(len(voxel)):\n",
    "                     ctx = F.pad(prior_out[[i]].cpu(), (0, 1664 - prior_out.shape[-1])).to(device).half()\n",
    "                     current_vector_suffix = vector_suffix_gpu[[i]]\n",
    "\n",
    "                     samples = utils.unclip_recon(ctx, diffusion_engine, current_vector_suffix, num_samples=num_samples_per_image)\n",
    "                     batch_recons.append(samples.float().cpu()) # Convert back to FP32 on CPU\n",
    "                     del ctx; del samples; del current_vector_suffix\n",
    "                     torch.cuda.empty_cache()\n",
    "\n",
    "            if batch_recons:\n",
    "                 batch_recons_tensor = torch.cat(batch_recons, dim=0)\n",
    "                 if all_recons is None: all_recons = batch_recons_tensor\n",
    "                 else: all_recons = torch.vstack((all_recons, batch_recons_tensor))\n",
    "                 reconstruction_succeeded = True\n",
    "\n",
    "            if batch_recons_tensor is not None: del batch_recons_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reconstruction: {e}\")\n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print(\"\\\\n--- CUDA Out of Memory ---\")\n",
    "                print(f\"Current Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "                print(f\"Max Memory Allocated: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
    "            reconstruction_succeeded = False\n",
    "        finally:\n",
    "            print(\"Moving diffusion_engine back to CPU...\")\n",
    "            diffusion_engine.to(\"cpu\").float() # Move model back to CPU and restore FP32\n",
    "\n",
    "            if hasattr(diffusion_engine.denoiser, 'sigmas') and diffusion_engine.denoiser.sigmas is not None:\n",
    "                 diffusion_engine.denoiser.sigmas = diffusion_engine.denoiser.sigmas.cpu()\n",
    "            if vector_suffix_gpu is not None: del vector_suffix_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if reconstruction_succeeded: print(\"Reconstruction successful.\")\n",
    "        else: print(\"Reconstruction failed or was skipped.\")\n",
    "\n",
    "        # --- Cleanup prior_out after it's used for reconstruction padding ---\n",
    "        del prior_out\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --- Post-processing and Saving ---\n",
    "imsize = 256\n",
    "if all_recons is not None and len(all_recons) > 0:\n",
    "    print(f\"Resizing {len(all_recons)} reconstructions to {imsize}x{imsize}...\")\n",
    "    # Use antialias=True for better downsampling quality\n",
    "    all_recons = transforms.Resize((imsize, imsize), antialias=True)(all_recons.float())\n",
    "else: all_recons = None\n",
    "\n",
    "# --- Skip resizing/saving blurry recons if decoding was commented out ---\n",
    "if blurry_recon and all_blurryrecons is not None and len(all_blurryrecons) > 0:\n",
    "    print(f\"Resizing {len(all_blurryrecons)} blurry reconstructions to {imsize}x{imsize}...\")\n",
    "    all_blurryrecons = transforms.Resize((imsize, imsize), antialias=True)(all_blurryrecons.float())\n",
    "else: all_blurryrecons = None\n",
    "\n",
    "# Saving\n",
    "print(\"Saving outputs...\")\n",
    "output_dir = f\"evals/{model_name}\"\n",
    "print(f\"Saving outputs to directory: {output_dir}\")\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating output directory {output_dir}: {e}\")\n",
    "    # Decide how to handle this - maybe exit or fallback? For now, just print error.\n",
    "\n",
    "\n",
    "if all_recons is not None:\n",
    "    recon_path = os.path.join(output_dir, f\"{model_name}_all_recons.pt\")\n",
    "    print(f\"Shape of saved recons: {all_recons.shape}. Saving to {recon_path}...\")\n",
    "    try:\n",
    "        torch.save(all_recons, recon_path)\n",
    "        print(\"Recons saved successfully.\")\n",
    "    except Exception as e:\n",
    "         print(f\"!!! ERROR saving recons to {recon_path}: {e}\")\n",
    "else: print(\"Skipping saving of image reconstructions (all_recons is None).\")\n",
    "\n",
    "# --- Skip saving blurry recons if decoding was commented out ---\n",
    "if all_blurryrecons is not None:\n",
    "     blurry_path = os.path.join(output_dir, f\"{model_name}_all_blurryrecons.pt\")\n",
    "     print(f\"Shape of saved blurry recons: {all_blurryrecons.shape}. Saving to {blurry_path}...\")\n",
    "     try:\n",
    "         torch.save(all_blurryrecons, blurry_path)\n",
    "         print(\"Blurry recons saved successfully.\")\n",
    "     except Exception as e: print(f\"!!! ERROR saving blurry recons to {blurry_path}: {e}\")\n",
    "else: print(\"Skipping saving of blurry reconstructions.\")\n",
    "\n",
    "\n",
    "# --- Corrected Caption Saving ---\n",
    "# Ensure all_predcaptions is a list of strings before saving\n",
    "if all_predcaptions:\n",
    "     if not isinstance(all_predcaptions, list) or not all(isinstance(item, str) for item in all_predcaptions):\n",
    "          print(\"Warning: all_predcaptions not list of strings. Attempting conversion...\")\n",
    "          try: all_predcaptions = [str(item) for item in all_predcaptions]\n",
    "          except Exception as conv_e: print(f\"Error converting captions: {conv_e}. Skipping save.\"); all_predcaptions = None\n",
    "     if all_predcaptions:\n",
    "          caption_path = os.path.join(output_dir, f\"{model_name}_all_predcaptions.pt\")\n",
    "          print(f\"Saving {len(all_predcaptions)} predicted captions to {caption_path}...\")\n",
    "          try:\n",
    "               torch.save(all_predcaptions, caption_path)\n",
    "               print(\"Captions saved successfully.\")\n",
    "          except Exception as save_e:\n",
    "               print(f\"!!! Error saving captions with torch.save: {save_e}\")\n",
    "               print(\"Attempting fallback to .npy...\")\n",
    "               try:\n",
    "                    caption_path_npy = os.path.join(output_dir, f\"{model_name}_all_predcaptions.npy\")\n",
    "                    np.save(caption_path_npy, np.array(all_predcaptions, dtype=object))\n",
    "                    print(f\"Captions saved as fallback to {caption_path_npy}\")\n",
    "               except Exception as npy_save_e: print(f\"!!! Fallback saving as .npy also failed: {npy_save_e}\")\n",
    "else: print(\"Skipping saving of captions (list is empty).\")\n",
    "\n",
    "\n",
    "# --- Save Backbone and Prior_Out if accumulated ---\n",
    "if all_backbones is not None:\n",
    "    backbones_path = os.path.join(output_dir, f\"{model_name}_all_backbones.pt\")\n",
    "    print(f\"Shape of saved backbones: {all_backbones.shape}. Saving to {backbones_path}...\")\n",
    "    try:\n",
    "        torch.save(all_backbones, backbones_path)\n",
    "        print(\"Backbones saved successfully.\")\n",
    "    except Exception as e: print(f\"!!! ERROR saving backbones to {backbones_path}: {e}\")\n",
    "else: print(\"Skipping saving of backbones (all_backbones is None).\")\n",
    "\n",
    "if all_prior_out is not None:\n",
    "    prior_out_path = os.path.join(output_dir, f\"{model_name}_all_prior_out.pt\")\n",
    "    print(f\"Shape of saved prior_out: {all_prior_out.shape}. Saving to {prior_out_path}...\")\n",
    "    try:\n",
    "        torch.save(all_prior_out, prior_out_path)\n",
    "        print(\"Prior_out saved successfully.\")\n",
    "    except Exception as e: print(f\"!!! ERROR saving prior_out to {prior_out_path}: {e}\")\n",
    "else: print(\"Skipping saving of prior_out (all_prior_out is None).\")\n",
    "\n",
    "\n",
    "if all_clipvoxels is not None:\n",
    "    clipvoxels_path = os.path.join(output_dir, f\"{model_name}_all_clipvoxels.pt\")\n",
    "    print(f\"Shape of saved clipvoxels: {all_clipvoxels.shape}. Saving to {clipvoxels_path}...\")\n",
    "    try:\n",
    "        torch.save(all_clipvoxels, clipvoxels_path)\n",
    "        print(\"Clipvoxels saved successfully.\")\n",
    "    except Exception as e: print(f\"!!! ERROR saving clipvoxels to {clipvoxels_path}: {e}\")\n",
    "else: print(\"Skipping saving of clipvoxels (all_clipvoxels is None).\")\n",
    "\n",
    "\n",
    "print(f\"Saved outputs for {model_name} in {output_dir}\")\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9443c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
