{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "import inspect\n",
    "\n",
    "# sdxl unclip requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "accelerator = Accelerator(split_batches=True, mixed_precision='fp16')\n",
    "device = accelerator.device\n",
    "print(\"device:\", device)\n",
    "tag = 'last'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: fmri_model_v1_1ses_50ep\n",
      "--data_path=/workspace/MindEyeV2/MindEyeV2/src/data                     --cache_dir=/workspace/MindEyeV2/MindEyeV2/src/cache                     --model_name=fmri_model_v1_1ses_50ep --subj=1                     --hidden_dim=1024 --n_blocks=4 --new_test\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"fmri_model_v1_1ses_50ep\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path={os.getcwd()}/data \\\n",
    "                    --cache_dir={os.getcwd()}/cache \\\n",
    "                    --model_name={model_name} --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --new_test\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"model training configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"fmri_model_v1_1ses_50ep\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"path to where misc. files downloaded from huggingface are stored. defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\", exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voxels for subj01: 15724\n",
      "/workspace/MindEyeV2/MindEyeV2/src/data/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voxels = {}\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32_renorm.hdf5', 'r')\n",
    "betas = f['betas'][:]\n",
    "betas = torch.Tensor(betas).to(\"cpu\")\n",
    "num_voxels = betas[0].shape[-1]\n",
    "voxels[f'subj0{subj}'] = betas\n",
    "print(f\"num_voxels for subj0{subj}: {num_voxels}\")\n",
    "\n",
    "if not new_test:\n",
    "    if subj == 3:\n",
    "        num_test = 2113\n",
    "    elif subj == 4:\n",
    "        num_test = 1985\n",
    "    elif subj == 6:\n",
    "        num_test = 2113\n",
    "    elif subj == 8:\n",
    "        num_test = 1985\n",
    "    else:\n",
    "        num_test = 2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/0.tar\"\n",
    "else:\n",
    "    if subj == 3:\n",
    "        num_test = 2371\n",
    "    elif subj == 4:\n",
    "        num_test = 2188\n",
    "    elif subj == 6:\n",
    "        num_test = 2371\n",
    "    elif subj == 8:\n",
    "        num_test = 2188\n",
    "    else:\n",
    "        num_test = 3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/0.tar\"\n",
    "\n",
    "print(test_url)\n",
    "def my_split_by_node(urls): return urls\n",
    "test_data = wds.WebDataset(test_url, resampled=False, nodesplitter=my_split_by_node) \\\n",
    "    .decode(\"torch\") \\\n",
    "    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\") \\\n",
    "    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000 3000 1000\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "\n",
    "test_images_idx = []\n",
    "test_voxels_idx = []\n",
    "for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "    test_voxels = voxels[f'subj0{subj}'][behav[:, 0, 5].cpu().long()]\n",
    "    test_voxels_idx = np.append(test_images_idx, behav[:, 0, 5].cpu().numpy())\n",
    "    test_images_idx = np.append(test_images_idx, behav[:, 0, 0].cpu().numpy())\n",
    "test_images_idx = test_images_idx.astype(int)\n",
    "test_voxels_idx = test_voxels_idx.astype(int)\n",
    "\n",
    "assert (test_i + 1) * num_test == len(test_voxels) == len(test_images_idx)\n",
    "print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "Loading blurry recon model\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "50,076,528 total\n",
      "50,076,528 trainable\n",
      "param counts:\n",
      "66,178,928 total\n",
      "66,178,928 trainable\n",
      "param counts:\n",
      "55,096,640 total\n",
      "55,096,624 trainable\n",
      "param counts:\n",
      "121,275,568 total\n",
      "121,275,552 trainable\n",
      "\n",
      "---loading /workspace/MindEyeV2/MindEyeV2/train_logs/fmri_model_v1_1ses_50ep/last.pth ckpt---\n",
      "\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-B-32\",\n",
    "    version='openai',\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(\"cpu\")\n",
    "clip_seq_dim = 49\n",
    "clip_emb_dim = 768\n",
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D'] * 4,\n",
    "        up_block_types=['UpDecoderBlock2D'] * 4,\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=224,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(\"cpu\")\n",
    "    utils.count_params(autoenc)\n",
    "    print(\"loading blurry recon model\")\n",
    "\n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "        ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "\n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim)\n",
    "\n",
    "from diffusers.models.autoencoders.vae import Decoder\n",
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                              clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim // 52\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "    dim=out_dim,\n",
    "    depth=depth,\n",
    "    dim_head=dim_head,\n",
    "    heads=heads,\n",
    "    causal=False,\n",
    "    num_tokens=clip_seq_dim,\n",
    "    learned_query_mode=\"pos_emb\"\n",
    ")\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del checkpoint\n",
    "except:\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---attempting to load /workspace/MindEyeV2/MindEyeV2/train_logs/fmri_model_v1_1ses_50ep/last.pth checkpoint---\n",
      "\n",
      "Checkpoint loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 49\n",
    "clip_text_emb_dim = 768\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "\n",
    "clip_convert = CLIPConverter()\n",
    "\n",
    "expected_ckpt_path = os.path.join(cache_dir, \"epoch10.pth\")\n",
    "print(f\"\\n---attempting to load {outdir}/{tag}.pth checkpoint---\\n\")\n",
    "checkpoint_path = os.path.join(outdir, f\"{tag}.pth\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    print(f\"WARNING: Directory {outdir} doesn't exist! Creating it...\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"WARNING: Checkpoint file {checkpoint_path} not found!\")\n",
    "    print(\"Continuing without loading a model checkpoint. Results may not be meaningful.\")\n",
    "else:\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        del checkpoint\n",
    "        print(\"Checkpoint loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        try:\n",
    "            import deepspeed\n",
    "            if os.path.exists(os.path.join(outdir, tag)):\n",
    "                state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(\n",
    "                    checkpoint_dir=outdir, tag=tag)\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "                del state_dict\n",
    "                print(\"DeepSpeed checkpoint loaded successfully!\")\n",
    "            else:\n",
    "                print(f\"DeepSpeed checkpoint directory {os.path.join(outdir, tag)} not found!\")\n",
    "        except Exception as deep_e:\n",
    "            print(f\"DeepSpeed loading also failed: {deep_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Looking for unCLIP checkpoint at: /workspace/MindEyeV2/MindEyeV2/src/cache/unclip6_epoch0_step110000.ckpt\n",
      "Found unCLIP checkpoint at: /workspace/MindEyeV2/MindEyeV2/src/cache/unclip6_epoch0_step110000.ckpt\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "# ───── create the engine exactly as before ─────────────────────────\n",
    "diffusion_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config,\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast,\n",
    ")\n",
    "\n",
    "# NEW ↓ — cast weights to fp16 to cut memory in half\n",
    "# diffusion_engine.half()\n",
    "\n",
    "# set to inference and put on CPU\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(\"cpu\")\n",
    "\n",
    "# With these lines\n",
    "ckpt_path = os.path.join(cache_dir, \"unclip6_epoch0_step110000.ckpt\")\n",
    "print(f\"Looking for unCLIP checkpoint at: {ckpt_path}\")\n",
    "\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print(f\"ERROR: unCLIP checkpoint not found at {ckpt_path}\")\n",
    "    print(\"You need to download the unCLIP model checkpoint first.\")\n",
    "    print(\"This is typically available from Hugging Face or the model creator's repository.\")\n",
    "    print(\"After downloading, place it in your cache directory, which is set to:\")\n",
    "    print(f\"  {cache_dir}\")\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    raise FileNotFoundError(f\"Missing required checkpoint: {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"Found unCLIP checkpoint at: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    diffusion_engine.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure blurry_recon argument is used ---\n",
    "print(f\"Blurry Reconstruction Flag (from args): {blurry_recon}\")\n",
    "\n",
    "# (vector_suffix inference logic - keep the corrected version)\n",
    "try:\n",
    "    embed_dim = None; embedder_info = []\n",
    "    if hasattr(diffusion_engine, 'conditioner') and hasattr(diffusion_engine.conditioner, 'embedders'):\n",
    "        print(\"Available conditioner embedders:\")\n",
    "        for i, emb in enumerate(diffusion_engine.conditioner.embedders):\n",
    "            emb_type = type(emb); embedder_info.append(f\"  Index: {i}, Type: {emb_type}\")\n",
    "            if isinstance(emb, FrozenOpenCLIPEmbedder2):\n",
    "                proj_layer = getattr(getattr(emb, 'model', None), 'text_projection', None)\n",
    "                if proj_layer is not None and hasattr(proj_layer, 'shape'):\n",
    "                    embed_dim = proj_layer.shape[-1]\n",
    "                    embedder_info[-1] += f\" -> Found embed_dim: {embed_dim} (Preferred)\"\n",
    "                    break\n",
    "                else:\n",
    "                    embedder_info[-1] += \" -> Warning: Could not get text_projection shape\"\n",
    "            elif isinstance(emb, FrozenOpenCLIPImageEmbedder):\n",
    "                found_dim = getattr(emb, 'output_dim', getattr(getattr(emb, 'model', None), 'output_dim', None))\n",
    "                if found_dim and embed_dim is None:\n",
    "                    embed_dim = found_dim\n",
    "                    embedder_info[-1] += f\" -> Found embed_dim: {found_dim}\"\n",
    "    for info in embedder_info: print(info)\n",
    "    vector_suffix_shape = (1, embed_dim) if isinstance(embed_dim, int) else (1, 1024)\n",
    "    print(f\"Using vector_suffix_shape: {vector_suffix_shape}\")\n",
    "    vector_suffix = torch.zeros(vector_suffix_shape, device='cpu', dtype=torch.float32)\n",
    "    print(f\"Created placeholder vector_suffix with shape: {vector_suffix.shape}. Needs verification!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inferring vector_suffix shape: {e}. Using default (1, 1024).\")\n",
    "    vector_suffix = torch.zeros(1, 1024, device='cpu', dtype=torch.float32)\n",
    "\n",
    "# get all reconstructions\n",
    "# get all reconstructions\n",
    "model.to(\"cpu\")\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# --- Load Autoencoder Conditionally ---\n",
    "autoenc = None # Ensure initialized to None\n",
    "if blurry_recon:\n",
    "    print(\"Attempting to load Autoencoder for Blurry Recon...\")\n",
    "    try:\n",
    "        from diffusers import AutoencoderKL\n",
    "        autoenc = AutoencoderKL(\n",
    "             down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "             up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "             block_out_channels=[128, 256, 512, 512], layers_per_block=2, sample_size=256,\n",
    "         )\n",
    "        ckpt_path = f'{cache_dir}/sd_image_var_autoenc.pth'\n",
    "        if not os.path.exists(ckpt_path):\n",
    "             print(f\"ERROR: Autoencoder checkpoint file not found at {ckpt_path}. Disabling blurry recon.\")\n",
    "             blurry_recon = False # Turn off flag if file missing\n",
    "             autoenc = None\n",
    "        else:\n",
    "             ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "             autoenc.load_state_dict(ckpt)\n",
    "             print(f\"Autoencoder loaded successfully from {ckpt_path}\")\n",
    "             autoenc.eval().requires_grad_(False).to(\"cpu\") # Keep on CPU initially\n",
    "             utils.count_params(autoenc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading autoencoder: {e}. Disabling blurry recon.\")\n",
    "        blurry_recon = False # Turn off flag on error\n",
    "        autoenc = None\n",
    "else:\n",
    "    print(\"Blurry reconstruction disabled by args, skipping Autoencoder load.\")\n",
    "\n",
    "# Keep other models on CPU initially\n",
    "clip_text_model.to(\"cpu\")\n",
    "diffusion_engine.to(\"cpu\")\n",
    "\n",
    "# Initialize result accumulators\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = [] # Use a standard list\n",
    "all_clipvoxels = None\n",
    "\n",
    "# Training Loop Settings\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "assert num_samples_per_image == 1\n",
    "if utils.is_interactive(): plotting=False\n",
    "dtype = torch.float16\n",
    "\n",
    "# Check BrainNetwork internal flag\n",
    "if hasattr(model, 'backbone') and hasattr(model.backbone, 'blurry_recon'):\n",
    "     print(f\"Loaded model.backbone internal blurry_recon flag: {model.backbone.blurry_recon}\")\n",
    "else: print(\"Warning: Cannot check internal blurry_recon flag.\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_images_idx_unique = np.unique(test_images_idx)\n",
    "    test_images_idx_unique = test_images_idx_unique[:10] # Limit for debugging\n",
    "\n",
    "    print(f\"Processing {len(test_images_idx_unique)} unique images...\")\n",
    "    for batch_start in tqdm(range(0, len(test_images_idx_unique), minibatch_size)):\n",
    "        batch_end = batch_start + minibatch_size\n",
    "        uniq_imgs = test_images_idx_unique[batch_start:batch_end]\n",
    "\n",
    "        # --- Voxel Preparation (CPU) ---\n",
    "        voxel_list = []\n",
    "        for uniq_img in uniq_imgs:\n",
    "            locs = np.where(test_images_idx == uniq_img)[0]\n",
    "            if len(locs) == 1: locs = np.repeat(locs, 3)\n",
    "            elif len(locs) == 2: locs = np.concatenate([locs[[0]], locs])[:3]\n",
    "            elif len(locs) > 3: locs = locs[:3]\n",
    "            elif len(locs) < 1: continue\n",
    "            assert len(locs) == 3\n",
    "            max_voxel_idx = len(test_voxels) - 1\n",
    "            valid_locs = [l for l in locs if l <= max_voxel_idx]\n",
    "            if len(valid_locs) != 3: continue\n",
    "            voxel_list.append(test_voxels[None, valid_locs])\n",
    "        if not voxel_list: continue\n",
    "        voxel = torch.cat(voxel_list, dim=0).to(\"cpu\")\n",
    "\n",
    "        # --- MindEye Processing (CPU) ---\n",
    "        all_backbone_reps = []\n",
    "        all_clip_voxels_reps = []\n",
    "        all_blurry_enc_reps = []\n",
    "        for rep in range(3):\n",
    "            voxel_ridge = model.ridge(voxel[:, [rep]], 0)\n",
    "            backbone_out = model.backbone(voxel_ridge)\n",
    "            # Robust Output Handling\n",
    "            backbone_rep, clip_voxels_rep, blurry_latent_rep = None, None, None\n",
    "            if isinstance(backbone_out, (list, tuple)) and len(backbone_out) >= 2:\n",
    "                 backbone_rep = backbone_out[0]\n",
    "                 clip_voxels_rep = backbone_out[1]\n",
    "                 # --- Get Blurry Component Correctly ---\n",
    "                 if len(backbone_out) >= 3:\n",
    "                     blurry_component = backbone_out[2]\n",
    "                     # Check if the component itself is the tensor or a tuple containing it\n",
    "                     if isinstance(blurry_component, torch.Tensor):\n",
    "                          blurry_latent_rep = blurry_component\n",
    "                     elif isinstance(blurry_component, tuple) and len(blurry_component) > 0 and isinstance(blurry_component[0], torch.Tensor):\n",
    "                          # Assuming the first element of the tuple is the desired latent\n",
    "                          blurry_latent_rep = blurry_component[0]\n",
    "                     else:\n",
    "                          blurry_latent_rep = None\n",
    "                 else:\n",
    "                      blurry_latent_rep = None # No third element returned\n",
    "            else:\n",
    "                 print(\"Warning: Unexpected output structure from model.backbone.\")\n",
    "                 backbone_rep = backbone_out\n",
    "                 clip_voxels_rep = torch.zeros_like(backbone_rep) if isinstance(backbone_rep, torch.Tensor) else None\n",
    "                 blurry_latent_rep = None\n",
    "            all_backbone_reps.append(backbone_rep)\n",
    "            all_clip_voxels_reps.append(clip_voxels_rep)\n",
    "            all_blurry_enc_reps.append(blurry_latent_rep)\n",
    "\n",
    "        # Average - Check for None\n",
    "        backbone = torch.mean(torch.stack([t.cpu() for t in all_backbone_reps if t is not None]), dim=0) if all(t is not None for t in all_backbone_reps) else None\n",
    "        clip_voxels = torch.mean(torch.stack([t.cpu() for t in all_clip_voxels_reps if t is not None]), dim=0) if all(t is not None for t in all_clip_voxels_reps) else None\n",
    "        blurry_image_enc = None\n",
    "        if blurry_recon and all(isinstance(t, torch.Tensor) for t in all_blurry_enc_reps):\n",
    "            try:\n",
    "                 blurry_tensors_for_stacking = [t.cpu() for t in all_blurry_enc_reps]\n",
    "                 stacked_blurry_encs = torch.stack(blurry_tensors_for_stacking, dim=1) # Stack along dim 1 (reps)\n",
    "                 blurry_image_enc = torch.mean(stacked_blurry_encs, dim=1) # Average over dim 1\n",
    "                 print(f\"Averaged blurry latent shape: {blurry_image_enc.shape}\")\n",
    "            except Exception as e: print(f\"Error averaging blurry latents: {e}. Setting to None.\")\n",
    "        elif blurry_recon: print(\"Warning: Blurry latents not valid tensors.\")\n",
    "\n",
    "        if backbone is None or clip_voxels is None: print(\"Skipping batch due to None backbone/clip_voxels.\"); continue\n",
    "\n",
    "        # Store clipvoxels\n",
    "        if all_clipvoxels is None: all_clipvoxels = clip_voxels.cpu()\n",
    "        else: all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "\n",
    "        # Diffusion Prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, text_cond=dict(text_embed=backbone), cond_scale=1., timesteps=20)\n",
    "\n",
    "        # Caption Generation\n",
    "        try:\n",
    "            pred_caption_emb = clip_convert(prior_out)\n",
    "            generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_predcaptions.extend(generated_caption)\n",
    "            print(f\"Batch {batch_start//minibatch_size}: {generated_caption}\")\n",
    "        except Exception as caption_e:\n",
    "            print(f\"Error during caption generation: {caption_e}\")\n",
    "            all_predcaptions.extend([\"<caption_error>\"] * len(voxel))\n",
    "\n",
    "        # --- unCLIP Reconstruction (Attempt on GPU) ---\n",
    "        vector_suffix_gpu = None; batch_recons_tensor = None; ctx = None; samples = None\n",
    "        reconstruction_succeeded = False\n",
    "        try:\n",
    "            print(\"Attempting reconstruction: Moving diffusion_engine to GPU...\")\n",
    "            diffusion_engine.to(device)\n",
    "            if hasattr(diffusion_engine.denoiser, 'sigmas') and diffusion_engine.denoiser.sigmas is not None:\n",
    "                 diffusion_engine.denoiser.sigmas = diffusion_engine.denoiser.sigmas.to(device)\n",
    "            current_batch_size = len(voxel)\n",
    "            vs_repeated = vector_suffix.repeat(current_batch_size, 1) if vector_suffix.shape[0] != current_batch_size else vector_suffix\n",
    "            vector_suffix_gpu = vs_repeated.to(device)\n",
    "            print(\"Generating reconstructions...\")\n",
    "            batch_recons = []\n",
    "            with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "                 for i in range(len(voxel)):\n",
    "                     ctx = F.pad(prior_out[[i]], (0, 1664 - clip_emb_dim)).to(device)\n",
    "                     samples = utils.unclip_recon(ctx, diffusion_engine, vector_suffix_gpu[[i]], num_samples=num_samples_per_image)\n",
    "                     batch_recons.append(samples.cpu())\n",
    "                     del ctx; del samples\n",
    "            if batch_recons:\n",
    "                 batch_recons_tensor = torch.cat(batch_recons, dim=0)\n",
    "                 if all_recons is None: all_recons = batch_recons_tensor\n",
    "                 else: all_recons = torch.vstack((all_recons, batch_recons_tensor))\n",
    "                 reconstruction_succeeded = True\n",
    "            del batch_recons;\n",
    "            if batch_recons_tensor is not None: del batch_recons_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reconstruction: {e}\")\n",
    "            if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory ---\\n\")\n",
    "            reconstruction_succeeded = False\n",
    "        finally:\n",
    "            print(\"Moving diffusion_engine back to CPU...\")\n",
    "            diffusion_engine.to(\"cpu\")\n",
    "            if vector_suffix_gpu is not None: del vector_suffix_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "        if reconstruction_succeeded: print(\"Reconstruction successful.\")\n",
    "\n",
    "        # --- Blurry Reconstruction (Attempt on GPU IF ENABLED and Latent Available) ---\n",
    "        if blurry_recon and autoenc is not None and blurry_image_enc is not None:\n",
    "             blurry_image_enc_gpu = None; blurred_image_gpu = None; blurred_image = None\n",
    "             try:\n",
    "                print(\"Attempting blurry reconstruction: Moving autoenc to GPU...\")\n",
    "                autoenc.to(device)\n",
    "                blurry_image_enc_gpu = blurry_image_enc.to(device)\n",
    "                print(f\"Shape feeding into blurry decode: {blurry_image_enc_gpu.shape}\")\n",
    "\n",
    "                print(\"Decoding blurry images...\")\n",
    "                with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "                     scale_factor_vae = getattr(getattr(autoenc, 'config', None), 'scaling_factor', 0.18215)\n",
    "                     blurred_image_gpu = (autoenc.decode(blurry_image_enc_gpu / scale_factor_vae).sample / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "                blurred_image = blurred_image_gpu.cpu()\n",
    "                print(\"Blurry decoding done, images moved to CPU.\")\n",
    "\n",
    "                if all_blurryrecons is None: all_blurryrecons = blurred_image\n",
    "                else: all_blurryrecons = torch.vstack((all_blurryrecons, blurred_image))\n",
    "\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during blurry reconstruction: {e}\")\n",
    "                 if 'Expected' in str(e) and 'input' in str(e): print(\">>> Shape mismatch feeding into autoenc.decode.\")\n",
    "                 if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory during Blurry Recon ---\\n\")\n",
    "             finally:\n",
    "                print(\"Moving autoenc back to CPU...\")\n",
    "                autoenc.to(\"cpu\")\n",
    "                if blurry_image_enc_gpu is not None: del blurry_image_enc_gpu\n",
    "                if blurred_image_gpu is not None: del blurred_image_gpu\n",
    "                torch.cuda.empty_cache()\n",
    "        elif blurry_recon:\n",
    "             print(\"Skipping blurry reconstruction for this batch (autoenc or latent missing/invalid).\")\n",
    "\n",
    "        # End of loop cleanup\n",
    "        del voxel, voxel_list, voxel_ridge, backbone, clip_voxels, blurry_image_enc\n",
    "        del all_backbone_reps, all_clip_voxels_reps, all_blurry_enc_reps\n",
    "        del prior_out\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_images_idx_unique = np.unique(test_images_idx)\n",
    "    test_images_idx_unique = test_images_idx_unique[:10] # Limit for debugging\n",
    "\n",
    "    print(f\"Processing {len(test_images_idx_unique)} unique images...\")\n",
    "    for batch_start in tqdm(range(0, len(test_images_idx_unique), minibatch_size)):\n",
    "        batch_end = batch_start + minibatch_size\n",
    "        uniq_imgs = test_images_idx_unique[batch_start:batch_end]\n",
    "\n",
    "        # --- Voxel Preparation (CPU) ---\n",
    "        voxel_list = []\n",
    "        for uniq_img in uniq_imgs:\n",
    "            locs = np.where(test_images_idx == uniq_img)[0]\n",
    "            if len(locs) == 1: locs = np.repeat(locs, 3)\n",
    "            elif len(locs) == 2: locs = np.concatenate([locs[[0]], locs])[:3]\n",
    "            elif len(locs) > 3: locs = locs[:3]\n",
    "            elif len(locs) < 1: continue\n",
    "            assert len(locs) == 3\n",
    "            max_voxel_idx = len(test_voxels) - 1\n",
    "            valid_locs = [l for l in locs if l <= max_voxel_idx]\n",
    "            if len(valid_locs) != 3: continue\n",
    "            voxel_list.append(test_voxels[None, valid_locs])\n",
    "        if not voxel_list: continue\n",
    "        voxel = torch.cat(voxel_list, dim=0).to(\"cpu\")\n",
    "\n",
    "        # --- MindEye Processing (CPU) ---\n",
    "        all_backbone_reps = []\n",
    "        all_clip_voxels_reps = []\n",
    "        all_blurry_enc_reps = []\n",
    "        for rep in range(3):\n",
    "            voxel_ridge = model.ridge(voxel[:, [rep]], 0)\n",
    "            backbone_out = model.backbone(voxel_ridge)\n",
    "            # Robust Output Handling\n",
    "            backbone_rep, clip_voxels_rep, blurry_latent_rep = None, None, None\n",
    "            if isinstance(backbone_out, (list, tuple)) and len(backbone_out) >= 2:\n",
    "                 backbone_rep = backbone_out[0]\n",
    "                 clip_voxels_rep = backbone_out[1]\n",
    "                 # --- Get Blurry Component Correctly ---\n",
    "                 if len(backbone_out) >= 3:\n",
    "                     blurry_component = backbone_out[2]\n",
    "                     # Check if the component itself is the tensor or a tuple containing it\n",
    "                     if isinstance(blurry_component, torch.Tensor):\n",
    "                          blurry_latent_rep = blurry_component\n",
    "                     elif isinstance(blurry_component, tuple) and len(blurry_component) > 0 and isinstance(blurry_component[0], torch.Tensor):\n",
    "                          # Assuming the first element of the tuple is the desired latent\n",
    "                          blurry_latent_rep = blurry_component[0]\n",
    "                     else:\n",
    "                          blurry_latent_rep = None\n",
    "                 else:\n",
    "                      blurry_latent_rep = None # No third element returned\n",
    "            else:\n",
    "                 print(\"Warning: Unexpected output structure from model.backbone.\")\n",
    "                 backbone_rep = backbone_out\n",
    "                 clip_voxels_rep = torch.zeros_like(backbone_rep) if isinstance(backbone_rep, torch.Tensor) else None\n",
    "                 blurry_latent_rep = None\n",
    "            all_backbone_reps.append(backbone_rep)\n",
    "            all_clip_voxels_reps.append(clip_voxels_rep)\n",
    "            all_blurry_enc_reps.append(blurry_latent_rep)\n",
    "\n",
    "        # Average - Check for None\n",
    "        backbone = torch.mean(torch.stack([t.cpu() for t in all_backbone_reps if t is not None]), dim=0) if all(t is not None for t in all_backbone_reps) else None\n",
    "        clip_voxels = torch.mean(torch.stack([t.cpu() for t in all_clip_voxels_reps if t is not None]), dim=0) if all(t is not None for t in all_clip_voxels_reps) else None\n",
    "        blurry_image_enc = None\n",
    "        if blurry_recon and all(isinstance(t, torch.Tensor) for t in all_blurry_enc_reps):\n",
    "            try:\n",
    "                 blurry_tensors_for_stacking = [t.cpu() for t in all_blurry_enc_reps]\n",
    "                 stacked_blurry_encs = torch.stack(blurry_tensors_for_stacking, dim=1) # Stack along dim 1 (reps)\n",
    "                 blurry_image_enc = torch.mean(stacked_blurry_encs, dim=1) # Average over dim 1\n",
    "                 print(f\"Averaged blurry latent shape: {blurry_image_enc.shape}\")\n",
    "            except Exception as e: print(f\"Error averaging blurry latents: {e}. Setting to None.\")\n",
    "        elif blurry_recon: print(\"Warning: Blurry latents not valid tensors.\")\n",
    "\n",
    "        if backbone is None or clip_voxels is None: print(\"Skipping batch due to None backbone/clip_voxels.\"); continue\n",
    "\n",
    "        # Store clipvoxels\n",
    "        if all_clipvoxels is None: all_clipvoxels = clip_voxels.cpu()\n",
    "        else: all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "\n",
    "        # Diffusion Prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, text_cond=dict(text_embed=backbone), cond_scale=1., timesteps=20)\n",
    "\n",
    "        # Caption Generation\n",
    "        try:\n",
    "            pred_caption_emb = clip_convert(prior_out)\n",
    "            generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            all_predcaptions.extend(generated_caption)\n",
    "            print(f\"Batch {batch_start//minibatch_size}: {generated_caption}\")\n",
    "        except Exception as caption_e:\n",
    "            print(f\"Error during caption generation: {caption_e}\")\n",
    "            all_predcaptions.extend([\"<caption_error>\"] * len(voxel))\n",
    "\n",
    "        # --- unCLIP Reconstruction (Attempt on GPU) ---\n",
    "        vector_suffix_gpu = None; batch_recons_tensor = None; ctx = None; samples = None\n",
    "        reconstruction_succeeded = False\n",
    "        try:\n",
    "            print(\"Attempting reconstruction: Moving diffusion_engine to GPU...\")\n",
    "            diffusion_engine.to(device)\n",
    "            if hasattr(diffusion_engine.denoiser, 'sigmas') and diffusion_engine.denoiser.sigmas is not None:\n",
    "                 diffusion_engine.denoiser.sigmas = diffusion_engine.denoiser.sigmas.to(device)\n",
    "            current_batch_size = len(voxel)\n",
    "            vs_repeated = vector_suffix.repeat(current_batch_size, 1) if vector_suffix.shape[0] != current_batch_size else vector_suffix\n",
    "            vector_suffix_gpu = vs_repeated.to(device)\n",
    "            print(\"Generating reconstructions...\")\n",
    "            batch_recons = []\n",
    "            with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "                 for i in range(len(voxel)):\n",
    "                     ctx = F.pad(prior_out[[i]], (0, 1664 - clip_emb_dim)).to(device)\n",
    "                     samples = utils.unclip_recon(ctx, diffusion_engine, vector_suffix_gpu[[i]], num_samples=num_samples_per_image)\n",
    "                     batch_recons.append(samples.cpu())\n",
    "                     del ctx; del samples\n",
    "            if batch_recons:\n",
    "                 batch_recons_tensor = torch.cat(batch_recons, dim=0)\n",
    "                 if all_recons is None: all_recons = batch_recons_tensor\n",
    "                 else: all_recons = torch.vstack((all_recons, batch_recons_tensor))\n",
    "                 reconstruction_succeeded = True\n",
    "            del batch_recons;\n",
    "            if batch_recons_tensor is not None: del batch_recons_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reconstruction: {e}\")\n",
    "            if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory ---\\n\")\n",
    "            reconstruction_succeeded = False\n",
    "        finally:\n",
    "            print(\"Moving diffusion_engine back to CPU...\")\n",
    "            diffusion_engine.to(\"cpu\")\n",
    "            if vector_suffix_gpu is not None: del vector_suffix_gpu\n",
    "            torch.cuda.empty_cache()\n",
    "        if reconstruction_succeeded: print(\"Reconstruction successful.\")\n",
    "\n",
    "        # --- Blurry Reconstruction (Attempt on GPU IF ENABLED and Latent Available) ---\n",
    "        if blurry_recon and autoenc is not None and blurry_image_enc is not None:\n",
    "             blurry_image_enc_gpu = None; blurred_image_gpu = None; blurred_image = None\n",
    "             try:\n",
    "                print(\"Attempting blurry reconstruction: Moving autoenc to GPU...\")\n",
    "                autoenc.to(device)\n",
    "                blurry_image_enc_gpu = blurry_image_enc.to(device)\n",
    "                print(f\"Shape feeding into blurry decode: {blurry_image_enc_gpu.shape}\")\n",
    "\n",
    "                print(\"Decoding blurry images...\")\n",
    "                with torch.cuda.amp.autocast(dtype=dtype, enabled=(dtype != torch.float32)):\n",
    "                     scale_factor_vae = getattr(getattr(autoenc, 'config', None), 'scaling_factor', 0.18215)\n",
    "                     blurred_image_gpu = (autoenc.decode(blurry_image_enc_gpu / scale_factor_vae).sample / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "                blurred_image = blurred_image_gpu.cpu()\n",
    "                print(\"Blurry decoding done, images moved to CPU.\")\n",
    "\n",
    "                if all_blurryrecons is None: all_blurryrecons = blurred_image\n",
    "                else: all_blurryrecons = torch.vstack((all_blurryrecons, blurred_image))\n",
    "\n",
    "             except Exception as e:\n",
    "                 print(f\"Error during blurry reconstruction: {e}\")\n",
    "                 if 'Expected' in str(e) and 'input' in str(e): print(\">>> Shape mismatch feeding into autoenc.decode.\")\n",
    "                 if 'out of memory' in str(e).lower(): print(\"\\n--- CUDA Out of Memory during Blurry Recon ---\\n\")\n",
    "             finally:\n",
    "                print(\"Moving autoenc back to CPU...\")\n",
    "                autoenc.to(\"cpu\")\n",
    "                if blurry_image_enc_gpu is not None: del blurry_image_enc_gpu\n",
    "                if blurred_image_gpu is not None: del blurred_image_gpu\n",
    "                torch.cuda.empty_cache()\n",
    "        elif blurry_recon:\n",
    "             print(\"Skipping blurry reconstruction for this batch (autoenc or latent missing/invalid).\")\n",
    "\n",
    "        # End of loop cleanup\n",
    "        del voxel, voxel_list, voxel_ridge, backbone, clip_voxels, blurry_image_enc\n",
    "        del all_backbone_reps, all_clip_voxels_reps, all_blurry_enc_reps\n",
    "        del prior_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9443c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
