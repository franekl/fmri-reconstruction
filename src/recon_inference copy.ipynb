{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import torch.nn.functional as F   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "accelerator = Accelerator(split_batches=False)\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "tag='last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: fmri_model_v1_1ses_50ep\n",
      "--data_path=/workspace/MindEyeV2/fmri-reconstruction/src/data                     --cache_dir=/workspace/MindEyeV2/fmri-reconstruction/src/cache                     --model_name=fmri_model_v1_1ses_50ep --subj=1                     --hidden_dim=1024 --n_blocks=4 --new_test\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"fmri_model_v1_1ses_50ep\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path={os.getcwd()}/data \\\n",
    "                    --cache_dir={os.getcwd()}/cache \\\n",
    "                    --model_name=fmri_model_v1_1ses_50ep --subj=1 \\\n",
    "                    --hidden_dim=1024 --n_blocks=4 --new_test\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_voxels for subj01: 15724\n",
      "/workspace/MindEyeV2/fmri-reconstruction/src/data/wds/subj01/new_test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voxels = {}\n",
    "# Load hdf5 data for betas\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32_renorm.hdf5', 'r')\n",
    "betas = f['betas'][:]\n",
    "betas = torch.Tensor(betas).to(\"cpu\")\n",
    "num_voxels = betas[0].shape[-1]\n",
    "voxels[f'subj0{subj}'] = betas\n",
    "print(f\"num_voxels for subj0{subj}: {num_voxels}\")\n",
    "\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "else: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "    \n",
    "print(test_url)\n",
    "def my_split_by_node(urls): return urls\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cbeea8-e95b-48d9-9bc2-91af260c93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3000 3000 1000\n"
     ]
    }
   ],
   "source": [
    "# Prep images but don't load them all to memory\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images']\n",
    "\n",
    "# Prep test voxels and indices of test images\n",
    "test_images_idx = []\n",
    "test_voxels_idx = []\n",
    "for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "    test_voxels = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "    test_voxels_idx = np.append(test_images_idx, behav[:,0,5].cpu().numpy())\n",
    "    test_images_idx = np.append(test_images_idx, behav[:,0,0].cpu().numpy())\n",
    "test_images_idx = test_images_idx.astype(int)\n",
    "test_voxels_idx = test_voxels_idx.astype(int)\n",
    "\n",
    "assert (test_i+1) * num_test == len(test_voxels) == len(test_images_idx)\n",
    "print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "50,076,528 total\n",
      "50,076,528 trainable\n",
      "param counts:\n",
      "66,178,928 total\n",
      "66,178,928 trainable\n",
      "param counts:\n",
      "55,096,640 total\n",
      "55,096,624 trainable\n",
      "param counts:\n",
      "121,275,568 total\n",
      "121,275,552 trainable\n",
      "\n",
      "---loading /workspace/MindEyeV2/fmri-reconstruction/train_logs/fmri_model_v1_1ses_50ep/last.pth ckpt---\n",
      "\n",
      "ckpt loaded!\n"
     ]
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    # arch=\"ViT-L-14\",\n",
    "    arch=\"ViT-B-32\",\n",
    "    version='openai',\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "# clip_img_embedder.to(\"cpu\")\n",
    "clip_img_embedder.to(\"cpu\")\n",
    "clip_seq_dim = 49\n",
    "clip_emb_dim = 768\n",
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    ckpt = torch.load(f'{cache_dir}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(\"cpu\")\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim)\n",
    "\n",
    "from diffusers.models.autoencoders.vae import Decoder\n",
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)\n",
    "# Load pretrained model ckpt\n",
    "outdir = f\"/teamspace/studios/this_studio/MindEyeV2/train_logs/{model_name}\"\n",
    "\n",
    "# With this\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')  # Go up one directory to find train_logs\n",
    "print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "try:\n",
    "    checkpoint = torch.load(outdir+f'/{tag}.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del checkpoint\n",
    "except: # probably ckpt is saved using deepspeed format\n",
    "    import deepspeed\n",
    "    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    del state_dict\n",
    "print(\"ckpt loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---attempting to load /workspace/MindEyeV2/fmri-reconstruction/train_logs/fmri_model_v1_1ses_50ep/last.pth checkpoint---\n",
      "\n",
      "Checkpoint loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-base-coco\")\n",
    "clip_text_model.to(\"cpu\") # we got OOM running this script so can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 49\n",
    "clip_text_emb_dim = 768\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "\n",
    "clip_convert = CLIPConverter()\n",
    "\n",
    "# --- FIX: Change the filename here ---\n",
    "# Make sure the file 'epoch8.pth' actually exists in your cache_dir\n",
    "expected_ckpt_path = os.path.join(cache_dir, \"epoch24.pth\")\n",
    "print(f\"\\n---attempting to load {outdir}/{tag}.pth checkpoint---\\n\")\n",
    "checkpoint_path = os.path.join(outdir, f\"{tag}.pth\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    print(f\"WARNING: Directory {outdir} doesn't exist! Creating it...\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"WARNING: Checkpoint file {checkpoint_path} not found!\")\n",
    "    print(\"Continuing without loading a model checkpoint. Results may not be meaningful.\")\n",
    "else:\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        del checkpoint\n",
    "        print(\"Checkpoint loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        try:\n",
    "            import deepspeed\n",
    "            if os.path.exists(os.path.join(outdir, tag)):\n",
    "                state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(\n",
    "                    checkpoint_dir=outdir, tag=tag)\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "                del state_dict\n",
    "                print(\"DeepSpeed checkpoint loaded successfully!\")\n",
    "            else:\n",
    "                print(f\"DeepSpeed checkpoint directory {os.path.join(outdir, tag)} not found!\")\n",
    "        except Exception as deep_e:\n",
    "            print(f\"DeepSpeed loading also failed: {deep_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Looking for unCLIP checkpoint at: /workspace/MindEyeV2/fmri-reconstruction/src/cache/unclip6_epoch0_step110000.ckpt\n",
      "Found unCLIP checkpoint at: /workspace/MindEyeV2/fmri-reconstruction/src/cache/unclip6_epoch0_step110000.ckpt\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "# ───── create the engine exactly as before ─────────────────────────\n",
    "diffusion_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config,\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast,\n",
    ")\n",
    "\n",
    "# NEW ↓ — cast weights to fp16 to cut memory in half\n",
    "# diffusion_engine.half()\n",
    "\n",
    "# set to inference and put on GPU\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(\"cpu\")\n",
    "\n",
    "\n",
    "# With these lines\n",
    "ckpt_path = os.path.join(cache_dir, \"unclip6_epoch0_step110000.ckpt\")\n",
    "print(f\"Looking for unCLIP checkpoint at: {ckpt_path}\")\n",
    "\n",
    "if not os.path.exists(ckpt_path):\n",
    "    print(f\"ERROR: unCLIP checkpoint not found at {ckpt_path}\")\n",
    "    print(\"You need to download the unCLIP model checkpoint first.\")\n",
    "    print(\"This is typically available from Hugging Face or the model creator's repository.\")\n",
    "    print(\"After downloading, place it in your cache directory, which is set to:\")\n",
    "    print(f\"  {cache_dir}\")\n",
    "    \n",
    "    # Create required directories\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    \n",
    "    # Optionally raise an exception to stop execution or provide continuation options\n",
    "    raise FileNotFoundError(f\"Missing required checkpoint: {ckpt_path}\")\n",
    "else:\n",
    "    print(f\"Found unCLIP checkpoint at: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    diffusion_engine.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857f1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a36d822e08247ee816bc6d838813d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You passed `inputs_embeds` to `.generate()`, but the model class GitForCausalLMClipEmb doesn't have its forwarding implemented. See the GPT2 implementation for an example (https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 88\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# print(pred_caption_emb.shape)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(pred_caption_emb\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     86\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,\n\u001b[1;32m     87\u001b[0m                     device\u001b[38;5;241m=\u001b[39mpred_caption_emb\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 88\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mclip_text_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_caption_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m generated_caption \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m all_predcaptions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((all_predcaptions, generated_caption))\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/transformers/generation/utils.py:1330\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1330\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_model_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fmri/lib/python3.10/site-packages/transformers/generation/utils.py:395\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m has_inputs_embeds_forwarding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    392\u001b[0m     inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_inputs_embeds_forwarding:\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `inputs_embeds` to `.generate()`, but the model class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have its forwarding implemented. See the GPT2 implementation for an example \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m     )\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# In this case, `input_ids` is moved to the `model_kwargs`, so a few automations (like the creation of\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# the attention mask) can rely on the actual model input.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_initialize_input_ids_for_generation(\n\u001b[1;32m    403\u001b[0m     inputs, bos_token_id, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs\n\u001b[1;32m    404\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: You passed `inputs_embeds` to `.generate()`, but the model class GitForCausalLMClipEmb doesn't have its forwarding implemented. See the GPT2 implementation for an example (https://github.com/huggingface/transformers/pull/21405), and feel free to open a PR with it!"
     ]
    }
   ],
   "source": [
    "# Modified version of the code in cell 11 to ensure device consistency\n",
    "\n",
    "# get all reconstructions\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# Move other models to the device\n",
    "clip_convert.to(device)  # Add this line\n",
    "clip_text_model.to(device)  # Change to use device instead of \"cpu\"\n",
    "\n",
    "# Ensure unclip components are on the right device\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "# Missing vector_suffix definition - add this before the loop\n",
    "# Infer the shape from the diffusion engine's embedders\n",
    "embed_dim = 768  # Default value based on your commented code\n",
    "vector_suffix = torch.zeros((1, embed_dim), device=device)\n",
    "\n",
    "# If using blurry_recon, also move autoenc to device\n",
    "if blurry_recon:\n",
    "    autoenc.to(device)\n",
    "\n",
    "# all_images = None\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "\n",
    "minibatch_size = 4\n",
    "num_samples_per_image = 1\n",
    "assert num_samples_per_image == 1\n",
    "\n",
    "if utils.is_interactive():\n",
    "    plotting = False  # Set default to False to avoid accidental plotting\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(range(0,len(np.unique(test_images_idx)),minibatch_size)):\n",
    "        uniq_imgs = np.unique(test_images_idx)[batch:batch+minibatch_size]\n",
    "        voxel = None\n",
    "        for uniq_img in uniq_imgs:\n",
    "            locs = np.where(test_images_idx==uniq_img)[0]\n",
    "            if len(locs)==1:\n",
    "                locs = locs.repeat(3)\n",
    "            elif len(locs)==2:\n",
    "                locs = locs.repeat(2)[:3]\n",
    "            assert len(locs)==3\n",
    "            if voxel is None:\n",
    "                voxel = test_voxels[None,locs] # 1, num_image_repetitions, num_voxels\n",
    "            else:\n",
    "                voxel = torch.vstack((voxel, test_voxels[None,locs]))\n",
    "        voxel = voxel.to(device)\n",
    "        \n",
    "        for rep in range(3):\n",
    "            voxel_ridge = model.ridge(voxel[:,[rep]],0) # 0th index of subj_list\n",
    "            backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)\n",
    "            if rep==0:\n",
    "                clip_voxels = clip_voxels0\n",
    "                backbone = backbone0\n",
    "                blurry_image_enc = blurry_image_enc0[0]\n",
    "            else:\n",
    "                clip_voxels += clip_voxels0\n",
    "                backbone += backbone0\n",
    "                blurry_image_enc += blurry_image_enc0[0]\n",
    "        clip_voxels /= 3\n",
    "        backbone /= 3\n",
    "        blurry_image_enc /= 3\n",
    "                \n",
    "        # Save retrieval submodule outputs\n",
    "        if all_clipvoxels is None:\n",
    "            all_clipvoxels = clip_voxels.cpu()\n",
    "        else:\n",
    "            all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "        \n",
    "        # Feed voxels through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)\n",
    "        \n",
    "        # Key fix: ensure prior_out is on the same device as clip_convert\n",
    "        prior_out = prior_out.to(device)  # Add this line\n",
    "        \n",
    "        pred_caption_emb = clip_convert(prior_out)\n",
    "        # print(pred_caption_emb.shape)\n",
    "        # generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        attention_mask = torch.ones(pred_caption_emb.size()[:2],\n",
    "                            dtype=torch.long,\n",
    "                            device=pred_caption_emb.device)\n",
    "        generated_ids = clip_text_model.generate(\n",
    "                inputs_embeds=pred_caption_emb,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=20\n",
    "            )\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(generated_caption)\n",
    "        \n",
    "        # Rest of the code remains the same...\n",
    "        \n",
    "        # Feed diffusion prior outputs through unCLIP\n",
    "        for i in range(len(voxel)):\n",
    "            samples = utils.unclip_recon(prior_out[[i]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=num_samples_per_image)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if blurry_recon:\n",
    "            blurred_image = (autoenc.decode(blurry_image_enc/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "            \n",
    "            for i in range(len(voxel)):\n",
    "                im = torch.Tensor(blurred_image[i])\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im[None].cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im[None].cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if plotting: \n",
    "            print(model_name)\n",
    "            err # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "# resize outputs before saving\n",
    "imsize = 256\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "if blurry_recon: \n",
    "    all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "        \n",
    "# saving\n",
    "print(all_recons.shape)\n",
    "# # You can find the all_images file on huggingface: https://huggingface.co/datasets/pscotti/mindeyev2/tree/main/evals\n",
    "# torch.save(all_images,\"evals/all_images.pt\") \n",
    "if blurry_recon:\n",
    "    torch.save(all_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons.pt\")\n",
    "torch.save(all_recons,f\"evals/{model_name}/{model_name}_all_recons.pt\")\n",
    "torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions.pt\")\n",
    "torch.save(all_clipvoxels,f\"evals/{model_name}/{model_name}_all_clipvoxels.pt\")\n",
    "print(f\"saved {model_name} outputs!\")\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb277cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
